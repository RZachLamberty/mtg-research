{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDH card pair prediction\n",
    "\n",
    "1. build train / test / dev data set\n",
    "    1. get EDH card pair recommendations from edhrec.com. these have prediction value 1\n",
    "    1. generate false pairs (prediction value 0) by randomly generating pairs\n",
    "    1. split, stratifying on card color identity, card type, rarity.\n",
    "    1. convert cards into sentences\n",
    "1. fine-tune\n",
    "    1. load pre-trained bert model on prediction task \"card a, card b --> {yes,no} was edh rec\n",
    "1. make deck predictions for one of my existing decks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from pyarrow.csv import ConvertOptions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (BertConfig, BertTokenizerFast,\n",
    "                          BertForNextSentencePrediction,\n",
    "                          DataCollatorWithPadding,\n",
    "                          EarlyStoppingCallback,\n",
    "                          PreTrainedModel, PreTrainedTokenizerFast,\n",
    "                          Trainer, TrainingArguments, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm getting tired of toggling cells from raw to python code, so here I'll put some flags, and then wrap entire sections in these flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CARD_SAMPLER_VERSION = 2\n",
    "\n",
    "IS_LOCAL_LAPTOP = False\n",
    "BUILD_TTV_DATASETS = False\n",
    "#USE_CARD_SAMPLER_V2 = False\n",
    "DO_FINE_TUNING = False\n",
    "USE_FINE_TUNED_MODEL = False\n",
    "CHECK_FINE_TUNED_MODEL_RESULTS = False\n",
    "MAKE_DECK_RECOMMENDATION_DATASETS = False\n",
    "DO_DECK_RECOMMENDATIONS = False\n",
    "CHECK_DECK_RECOMMENDATIONS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CARD_SAMPLER_V1 = CARD_SAMPLER_VERSION == 1\n",
    "USE_CARD_SAMPLER_V2 = CARD_SAMPLER_VERSION == 2\n",
    "USE_CARD_SAMPLER_V3 = CARD_SAMPLER_VERSION == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUILD_TTV_DATASETS:\n",
    "    assert IS_LOCAL_LAPTOP\n",
    "\n",
    "if CHECK_FINE_TUNED_MODEL_RESULTS:\n",
    "    assert USE_FINE_TUNED_MODEL\n",
    "\n",
    "if MAKE_DECK_RECOMMENDATION_DATASETS:\n",
    "    assert IS_LOCAL_LAPTOP\n",
    "\n",
    "if DO_DECK_RECOMMENDATIONS:\n",
    "    assert USE_FINE_TUNED_MODEL\n",
    "\n",
    "if CHECK_DECK_RECOMMENDATIONS:\n",
    "    assert USE_FINE_TUNED_MODEL\n",
    "    assert IS_LOCAL_LAPTOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build train / test / dev data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get EDH card pair recommendations from edhrec.com\n",
    "\n",
    "these will have prediction value 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_LOCAL_LAPTOP:\n",
    "    import mtg.cards\n",
    "    import mtg.extract.edhrec\n",
    "\n",
    "    edhrec_cards = (mtg.extract.edhrec.get_commanders_and_cards()\n",
    "                    [['name', 'commander']])\n",
    "    edhrec_cards.name = edhrec_cards.name.str.lower()\n",
    "    edhrec_cards.commander = edhrec_cards.commander.str.lower()\n",
    "\n",
    "edhrec_cards.head() if IS_LOCAL_LAPTOP else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_LOCAL_LAPTOP:\n",
    "    # most common cards\n",
    "    edhrec_cards.name.value_counts().plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_LOCAL_LAPTOP:\n",
    "    # most common cards\n",
    "    vc = edhrec_cards.name.value_counts()\n",
    "vc.head() if IS_LOCAL_LAPTOP else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_LOCAL_LAPTOP:\n",
    "    # most cards appear in only 1 commander recs, up to 500 cards appear\n",
    "    # in 10 commander recs\n",
    "    vc[vc <= 10].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we just ran with this, how many total pairs could we generate this way? basically, for every card in deck X, every other card is a valid pair. that's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_LOCAL_LAPTOP:\n",
    "    z = edhrec_cards.commander.value_counts()\n",
    "    print(f\"{int((z * (z - 1) / 2).sum()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at first I was going to say no way, buuuuuut it's actually not terrible... we want big data, after all\n",
    "\n",
    "we would need to generate about 32 mil negative labels if that were the dataset we were interested in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get all cards from mtgjson\n",
    "\n",
    "to generate false pairs we will randomly select from all cards. about 65% of all MTG cards are referenced on edhrec, but the rest are also, presumably, good choices for 0 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_LOCAL_LAPTOP:\n",
    "    cards = (mtg.cards.cards_df()\n",
    "             .sort_values(by=['name', 'multiverseId'], ascending=False)\n",
    "             .groupby('name')\n",
    "             .first())\n",
    "    \n",
    "    cards.index = cards.index.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before groupby().last(): 56_002, 78\n",
    "# after: 21_814, 77\n",
    "cards.shape if IS_LOCAL_LAPTOP else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_LOCAL_LAPTOP:\n",
    "    all_cards = set(cards.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can eventually use this dataframe to create a generator of true card pairs off of a single card anchor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split, stratifying on card color identity, card type, rarity.\n",
    "\n",
    "we will split on cards. this is actually tricky, right? it would be easy if we could just do a 95/5/5 and then there was enough pairing between 5s and other 5s to build an entire test / val set, but I actually suspect we might have a problem fielding that many extra records. oh well, I guess we'll tell in due time\n",
    "\n",
    "since we want to stratify on so many things, and we have a 2/3s chance of any card being in the true label, I actually think fully random sampling is approporiate. we can look at the breakdown of that by other features if we need to\n",
    "\n",
    "update: let's hold out the two most recent full sets (zendikar rising (`ZNR`) and ikoria: lair of behemoths (`IKO`)) as a \"new set generalization\" test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUILD_TTV_DATASETS:\n",
    "    new_sets = ['ZNR', 'IKO']\n",
    "    \n",
    "    new_set = cards[cards.setname.isin(new_sets)]\n",
    "    old_set = cards[~cards.setname.isin(new_sets)]\n",
    "    \n",
    "    cards_test = new_set.index.values\n",
    "    cards_train, cards_val = train_test_split(old_set.index.values, test_size=.05, random_state=1337)\n",
    "\n",
    "    print(f\"\"\"\n",
    "train: {cards_train.shape[0]}\n",
    "test: {cards_test.shape[0]}\n",
    "val: {cards_val.shape[0]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert cards into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmc_map = {0.0: 'zero',\n",
    "           0.5: 'one half',\n",
    "           1.0: 'one',\n",
    "           2.0: 'two',\n",
    "           3.0: 'three',\n",
    "           4.0: 'four',\n",
    "           5.0: 'five',\n",
    "           6.0: 'six',\n",
    "           7.0: 'seven',\n",
    "           8.0: 'eight',\n",
    "           9.0: 'nine',\n",
    "           10.0: 'ten',\n",
    "           11.0: 'eleven',\n",
    "           12.0: 'twelve',\n",
    "           13.0: 'thirteen',\n",
    "           14.0: 'fourteen',\n",
    "           15.0: 'fifteen',\n",
    "           16.0: 'sixteen',\n",
    "           1000000.0: 'one million', }\n",
    "\n",
    "\n",
    "color_map = {'W': 'white', 'U': 'blue', 'B': 'black', 'R': 'red', 'G': 'green'}\n",
    "\n",
    "\n",
    "def parse_mana_colors_from_cost(mc):\n",
    "    return ', '.join(color_map[c] for c in 'WUBRG' if c in (mc or ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert parse_mana_colors_from_cost('{2}{U}{U}{B}') == 'blue, black'\n",
    "assert parse_mana_colors_from_cost('{8}{W}{W}') == 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_card_text(card):\n",
    "    mana_color_str = parse_mana_colors_from_cost(card.manaCost)\n",
    "    cmc_str = f\"{cmc_map[card.convertedManaCost]} mana\"\n",
    "    \n",
    "    if mana_color_str != '':\n",
    "        mana_color_str = f' including {mana_color_str}'\n",
    "    \n",
    "    return (f\"for {cmc_str}{mana_color_str}, cast {card.type} {card.name}: {card.text}\"\n",
    "            .lower()\n",
    "            .replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_LOCAL_LAPTOP:\n",
    "    card_text = pd.DataFrame({'text': cards.apply(get_card_text, axis=1)})\n",
    "card_text.head(20) if IS_LOCAL_LAPTOP else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's just go with this, see how it works out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a `huggingface` `datasets`\n",
    "\n",
    "following along with the relatively simple example [here](https://github.com/huggingface/datasets/blob/master/datasets/squad/squad.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### custom dataset loader?\n",
    "\n",
    "meh let's try the `csv` loader first"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class Edhrec(datasets.GeneratorBasedBuilder):\n",
    "    raise NotImplementedError(\"havent written builder configs\")\n",
    "    BUILDER_CONFIGS = []\n",
    "    \n",
    "    def _info(self):\n",
    "        return datasets.DatasetInfo(\n",
    "            description=\"lol no thanks\",\n",
    "            features=datasets.Features({\"id\": datasets.Value('string'),\n",
    "                                        \"text_a\": datasets.Value('string'),\n",
    "                                        \"text_b\": datasets.Value('string'),\n",
    "                                        \"label\": datasets.Value(\"int32\"), }),\n",
    "            supervised_keys=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `csv` loader\n",
    "\n",
    "generate `csv`s the same way we were doing `parquet` (see appendix) and load those as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def card_sampler(cardlist, num_pairs_per_card=10, exclude_cards=None):\n",
    "    exclude_cards = exclude_cards or set()\n",
    "    np.random.seed(1337)\n",
    "    neg_card_options = np.array(list(all_cards.difference(exclude_cards)))\n",
    "    for card in tqdm(cardlist):\n",
    "        rec = card_text.loc[card]\n",
    "        commanders = edhrec_cards[edhrec_cards.name == card].commander.unique()\n",
    "        \n",
    "        # get positive card labels\n",
    "        pos_options = (edhrec_cards[edhrec_cards.commander.isin(commanders)\n",
    "                                    & (edhrec_cards.name != card)\n",
    "                                    & (~edhrec_cards.name.isin(exclude_cards))]\n",
    "                       .name.unique())\n",
    "\n",
    "        n_labels = min(num_pairs_per_card, pos_options.shape[0])\n",
    "        pos_cards = np.random.choice(pos_options,\n",
    "                                     size=n_labels,\n",
    "                                     replace=False)\n",
    "        \n",
    "        for pos_card in pos_cards:\n",
    "            try:\n",
    "                pos_text_rec = card_text.loc[pos_card]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    pos_text_rec = card_text.loc[pos_card.replace('//', '/')]\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            yield (*rec, *pos_text_rec, False)\n",
    "        \n",
    "        # get negative samples:\n",
    "        # start by getting 3x as many as we had before, and assume that this\n",
    "        # will net at least x that aren't positive label options. additionally,\n",
    "        # we are comfortable having a full set of negative labels even if we\n",
    "        # had few (or no!) positive labels\n",
    "        neg_cards = np.random.choice(neg_card_options,\n",
    "                                     size=3 * num_pairs_per_card,\n",
    "                                     replace=False)\n",
    "        \n",
    "        neg_cards = list(set(neg_cards).difference(pos_cards))[:n_labels]\n",
    "        for neg_card in neg_cards:\n",
    "            neg_text_rec = card_text.loc[neg_card]\n",
    "            yield (*rec, *neg_text_rec, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def card_sampler_v2(cardlist, num_pairs_per_card=100, exclude_cards=None):\n",
    "    exclude_cards = exclude_cards or set()\n",
    "    np.random.seed(1337)\n",
    "    \n",
    "    allowed_edhrec_cards_df = edhrec_cards[~edhrec_cards.name.isin(exclude_cards)]\n",
    "    allowed_non_edhrec_cards = np.array(list(all_cards.difference(exclude_cards)))\n",
    "    \n",
    "    # print(allowed_edhrec_cards_df.head())\n",
    "    # print(allowed_non_edhrec_cards)\n",
    "    \n",
    "    for name_a in tqdm(cardlist):\n",
    "        text_a = card_text.loc[name_a, 'text']\n",
    "        commanders = edhrec_cards[edhrec_cards.name == name_a].commander.unique()\n",
    "        \n",
    "        if commanders.shape[0] > 0:\n",
    "            commander_peers = (allowed_edhrec_cards_df\n",
    "                               [allowed_edhrec_cards_df.commander.isin(commanders)\n",
    "                                & (allowed_edhrec_cards_df.name != name_a)]\n",
    "                               .name.unique())\n",
    "            \n",
    "            commander_non_peers = (allowed_edhrec_cards_df\n",
    "                                   [(~allowed_edhrec_cards_df.commander.isin(commanders))\n",
    "                                    & (allowed_edhrec_cards_df.name != name_a)]\n",
    "                                   .name.unique())\n",
    "            \n",
    "            # true labels: true pairs from this card's commander peers\n",
    "            n_true = min(num_pairs_per_card, commander_peers.shape[0])\n",
    "            \n",
    "            true_recs = np.random.choice(commander_peers,\n",
    "                                         size=n_true,\n",
    "                                         replace=False)\n",
    "\n",
    "            # false labels, high info: any edhrec cards that aren't commander peers\n",
    "            n_false_high = min(num_pairs_per_card, commander_non_peers.shape[0])\n",
    "            false_recs = np.random.choice(commander_non_peers,\n",
    "                                          size=n_false_high,\n",
    "                                          replace=False)\n",
    "            \n",
    "            # false labels, low info: any non-edhrec cards\n",
    "            #   actually, just get these from the non-edhrec cards\n",
    "            \n",
    "            rec_iters = [(true_recs, 0, 'edh-edh'), (false_recs, 1, 'edh-not')]\n",
    "        else:\n",
    "            n_high = math.ceil(2 * num_pairs_per_card / 3)\n",
    "            n_low = num_pairs_per_card - n_high\n",
    "            \n",
    "            # false labels, high info: any edhrec card\n",
    "            high_recs = np.random.choice(allowed_edhrec_cards_df.name.unique(),\n",
    "                                         size=n_high,\n",
    "                                         replace=False)\n",
    "            \n",
    "            # false labels, low info: any non-edhrec cards\n",
    "            low_recs = np.random.choice(allowed_non_edhrec_cards,\n",
    "                                        size=n_low,\n",
    "                                        replace=False)\n",
    "            \n",
    "            rec_iters = [(high_recs, 1, 'not-edh'), (low_recs, 1, 'not-not')]\n",
    "        \n",
    "        for name_b_rec_set, label, rec_set_type in rec_iters:\n",
    "            for name_b in name_b_rec_set:\n",
    "                try:\n",
    "                    text_b = card_text.loc[name_b, 'text']\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        text_b = card_text.loc[name_b.replace('//', '/'), 'text']\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "                yield (name_a, text_a, name_b, text_b, label, rec_set_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouper(iterable, n, fillvalue=None):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return itertools.zip_longest(*args, fillvalue=fillvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('.', 'data_v2' if USE_CARD_SAMPLER_V2 else 'data')\n",
    "\n",
    "if BUILD_TTV_DATASETS:\n",
    "    \n",
    "    !rm -rf {data_dir}\n",
    "    \n",
    "    for split_type in ['train', 'test', 'val']:\n",
    "        os.makedirs(os.path.join(data_dir, split_type), exist_ok=True)\n",
    "        \n",
    "    !tree ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PAIRS_PER_CARD = 100\n",
    "PARQUET_CHUNK_SIZE = 100_000\n",
    "\n",
    "if BUILD_TTV_DATASETS:\n",
    "    columns = (['name_a', 'text_a', 'name_b', 'text_b', 'label', 'rec_set_type']\n",
    "               if USE_CARD_SAMPLER_V2\n",
    "               else ['text_a', 'text_b', 'label'])\n",
    "    \n",
    "    for (split_type, cardlist, exclude_cardlist) in [['val', cards_val, set(cards_test)],\n",
    "                                                     ['test', cards_test, None],\n",
    "                                                     ['train', cards_train, set(cards_val).union(cards_test)], ]:\n",
    "        \n",
    "        cs_foo = card_sampler_v2 if USE_CARD_SAMPLER_V2 else card_sampler\n",
    "        sampler = cs_foo(cardlist, NUM_PAIRS_PER_CARD, exclude_cardlist)\n",
    "        \n",
    "        for (i, grp) in enumerate(tqdm(grouper(sampler, PARQUET_CHUNK_SIZE))):\n",
    "            (pd.DataFrame(grp, columns=columns)\n",
    "             .dropna()\n",
    "             .to_csv(os.path.join(data_dir, split_type, f'part-{i:0>5}.csv'),\n",
    "                     index=False,\n",
    "                     quoting=csv.QUOTE_ALL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUILD_TTV_DATASETS:\n",
    "    !tree ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading csvs, shuffling, tokenizing, etc datasets now\n",
    "\n",
    "+ tokenizing from [here](https://huggingface.co/docs/datasets/processing.html#processing-data-in-batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_map_func(rec):\n",
    "    return tokenizer(rec['text_a'], rec['text_b'],\n",
    "                     padding='max_length',\n",
    "                     max_length=MAX_LENGTH,\n",
    "                     truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_label(rec):\n",
    "    return {'label_as_int': [int(_) for _ in rec['label']]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CARD_SAMPLER_V2:\n",
    "    features = datasets.Features({'name_a': datasets.Value(dtype='string'),\n",
    "                                  'text_a': datasets.Value(dtype='string'),\n",
    "                                  'name_b': datasets.Value(dtype='string'),\n",
    "                                  'text_b': datasets.Value(dtype='string'),\n",
    "                                  'label': datasets.Value(dtype='int64'),\n",
    "                                  'rec_set_type': datasets.Value(dtype='string'), })\n",
    "else:\n",
    "    features = datasets.Features({'text_a': datasets.Value(dtype='string'),\n",
    "                                  'text_b': datasets.Value(dtype='string'),\n",
    "                                  'label': datasets.Value(dtype='bool'), })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 300\n",
    "\n",
    "if DO_FINE_TUNING:\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    split_types = ['val', 'test', 'train']\n",
    "    \n",
    "    dataset = (load_dataset('csv',\n",
    "                            data_files={split_type: glob(os.path.join(data_dir, split_type, '*.csv'))\n",
    "                                        for split_type in split_types},\n",
    "                            quoting=csv.QUOTE_ALL,\n",
    "                            features=features)\n",
    "               #.map(fix_label,\n",
    "               #     batched=True)\n",
    "               .shuffle(seeds={split_type: 1337\n",
    "                               for split_type in split_types})\n",
    "               .map(tokenizer_map_func,\n",
    "                    batched=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset if DO_FINE_TUNING else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH = 36\n",
    "EVAL_BATCH = 36\n",
    "\n",
    "\n",
    "if DO_FINE_TUNING:\n",
    "    output_dir = './results_v2' if USE_CARD_SAMPLER_V2 else None\n",
    "    model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # # use this format to pick up from an aborted run\n",
    "    # model = BertForNextSentencePrediction.from_pretrained('./results_v2/checkpoint-5750')\n",
    "    \n",
    "    # this callback just says that if there are 8 consecutive (measured every\n",
    "    # eval_steps = ogging_steps steps) eval losses greater than the one at t0,\n",
    "    # kill the job\n",
    "    esc = EarlyStoppingCallback(early_stopping_patience=8,\n",
    "                                early_stopping_threshold=0.0)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,                    # output directory\n",
    "        num_train_epochs=1,                       # total # of training epochs\n",
    "        per_device_train_batch_size=TRAIN_BATCH,  # batch size per device during training\n",
    "        per_device_eval_batch_size=EVAL_BATCH,    # batch size for evaluation\n",
    "        warmup_steps=500,                         # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,                        # strength of weight decay\n",
    "        logging_dir='./logs',                     # directory for storing logs\n",
    "        # my custom ones\n",
    "        logging_steps=50,\n",
    "        overwrite_output_dir=True,\n",
    "        evaluation_strategy='steps',\n",
    "        logging_first_step=True,\n",
    "        seed=1337,\n",
    "        dataloader_drop_last=True,\n",
    "        dataloader_num_workers=30,\n",
    "        label_names=['labels'],\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=10,\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,                              # the instantiated 🤗 Transformers model to be trained\n",
    "        args=training_args,                       # training arguments, defined above\n",
    "        train_dataset=dataset['train'],           # training dataset\n",
    "        eval_dataset=dataset['val'], #.select(range(EVAL_BATCH * 1_000)),  # evaluation dataset\n",
    "        callbacks=[esc],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if DO_FINE_TUNING:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate() if DO_FINE_TUNING else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"edhrec-v2-bert-base-uncased\" if USE_CARD_SAMPLER_V2 else 'edhrec-bert-base-uncased'\n",
    "\n",
    "if DO_FINE_TUNING:\n",
    "    model.save_pretrained(model_name)\n",
    "    tokenizer.save_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run this if we did evaluate on only a sub-sample above\n",
    "# trainer.evaluate(dataset['val']) if DO_FINE_TUNING else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(dataset['test']) if DO_FINE_TUNING else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_FINE_TUNING:\n",
    "    p = trainer.predict(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test']['text_a'][0] if DO_FINE_TUNING else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test']['text_b'][0] if DO_FINE_TUNING else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test'][0]['label'] if DO_FINE_TUNING else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_FINE_TUNING:\n",
    "    zero_max = p.predictions[:, 0].argmax()\n",
    "\n",
    "p.predictions[zero_max] if DO_FINE_TUNING else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test']['text_a'][zero_max] if DO_FINE_TUNING else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test']['text_b'][zero_max] if DO_FINE_TUNING else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_FINE_TUNING:\n",
    "    one_max = p.predictions[:, 1].argmax()\n",
    "\n",
    "p.predictions[one_max] if DO_FINE_TUNING else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test']['text_a'][one_max] if DO_FINE_TUNING else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test']['text_b'][one_max] if DO_FINE_TUNING else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {k: np.array(v) for (k, v) in dataset['test'][:10].items() if k in ['attention_mask', 'input_ids', 'label', 'token_type_ids']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if DO_FINE_TUNING:\n",
    "    p = (model(**{k: torch.as_tensor(np.array(v)).to(\"cuda\")\n",
    "              for (k, v) in dataset['test'][:10].items()\n",
    "              if k in ['attention_mask', 'input_ids', 'token_type_ids']})\n",
    "     [0]\n",
    "     .softmax(1))\n",
    "\n",
    "p if DO_FINE_TUNING else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test']['label'][:10] if DO_FINE_TUNING else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## double-checking our trained model\n",
    "\n",
    "next steps\n",
    "\n",
    "+ what do our false positives look like\n",
    "+ what is the separation like for \"cards that have been on edhrec\" vs. \"cards that havent\n",
    "    + i.e. do we just predict \"both cards have been on EDHREC\"?\n",
    "    + did we create a dataset that is just (edhrec cards, either type)? I thought we were making (either type, either type)\n",
    "+ what is the sorted list of recommendations given an existing deck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why are these all only edhrec cards? I thought I was generating pairs from both sides?\n",
    "\n",
    "is this a problem? when a new card shows up and has never been seen before, will the model be unable to handle it? I think not, because presumably there were cards in test / val that it had never seen before (have I verified that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_FINE_TUNED_MODEL:\n",
    "    # loading the trained model\n",
    "    config = BertConfig.from_pretrained(model_name)\n",
    "    model = BertForNextSentencePrediction.from_pretrained(model_name, config=config)\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_BATCH = 36\n",
    "\n",
    "if USE_FINE_TUNED_MODEL:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./ignore',\n",
    "        per_device_eval_batch_size=EVAL_BATCH,    # batch size for evaluation\n",
    "        label_names=['labels'],\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(model=model, args=training_args, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 300\n",
    "\n",
    "if CHECK_FINE_TUNED_MODEL_RESULTS:\n",
    "    split_types = ['val', 'test']\n",
    "    \n",
    "    dataset = (load_dataset('csv',\n",
    "                            data_files={split_type: glob(os.path.join(data_dir, split_type, '*.csv'))\n",
    "                                        for split_type in split_types},\n",
    "                            quoting=csv.QUOTE_ALL,\n",
    "                            features=features)\n",
    "               #.map(fix_label,\n",
    "               #     batched=True)\n",
    "               .shuffle(seeds={split_type: 1337\n",
    "                               for split_type in split_types})\n",
    "               .map(tokenizer_map_func,\n",
    "                    batched=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHECK_FINE_TUNED_MODEL_RESULTS:\n",
    "    p = trainer.predict(dataset['test'].select(range(1_000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.predictions.shape if CHECK_FINE_TUNED_MODEL_RESULTS else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.label_ids.shape if CHECK_FINE_TUNED_MODEL_RESULTS else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.metrics if CHECK_FINE_TUNED_MODEL_RESULTS else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "if CHECK_FINE_TUNED_MODEL_RESULTS:\n",
    "    probs = softmax(p.predictions, axis=1)\n",
    "\n",
    "    z = pd.DataFrame({'p1': probs[:, 1],\n",
    "                      'y_pred': probs.argmax(axis=1),\n",
    "                      'y': p.label_ids})\n",
    "    z.loc[:, 'is_right'] = z.y == z.y_pred\n",
    "    z.loc[:, 'rec_set_type'] = dataset['test'].select(range(1_000))['rec_set_type']\n",
    "\n",
    "    z.sort_values(by='p1', ascending=False, inplace=True)\n",
    "    z.reset_index(drop=True, inplace=True)\n",
    "\n",
    "z.is_right.value_counts() if CHECK_FINE_TUNED_MODEL_RESULTS else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.groupby('rec_set_type').is_right.value_counts(normalize=True) if CHECK_FINE_TUNED_MODEL_RESULTS else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHECK_FINE_TUNED_MODEL_RESULTS:\n",
    "    total_true = z.y.sum()\n",
    "    (z.y.cumsum() / total_true).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHECK_FINE_TUNED_MODEL_RESULTS:\n",
    "    z.p1.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make deck predictions for one of my existing decks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "+ make the combo dataframe\n",
    "+ convert that into a dataset (probably a `.from_pandas` or some shit)\n",
    "+ pass that to eval\n",
    "+ sort by predictions\n",
    "+ profit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!ls -alh ../../mtg/mtg/tags/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!touch ~/.virtualenvs/mtg-research/src/mtg/mtg/tags.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the following on a computer with `mtg` installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_to_check = [\n",
    "    \"2XM\",\n",
    "    \"AKR\",\n",
    "    \"C20\",\n",
    "    \"CC1\",\n",
    "    \"CMC\",\n",
    "    \"CMR\",\n",
    "    \"IKO\",\n",
    "    \"JMP\",\n",
    "    \"KHC\",\n",
    "    \"M21\",\n",
    "    \"MB1\",\n",
    "    \"MH2\",\n",
    "    \"Q03\",\n",
    "    \"SLD\",\n",
    "    \"SLU\",\n",
    "    \"SS3\",\n",
    "    \"THB\",\n",
    "    \"TSR\",\n",
    "    \"ZNC\",\n",
    "    \"ZNE\",\n",
    "    \"ZNR\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_DECK_RECOMMENDATION_DATASETS:\n",
    "    from deck_lists import (esper_blink,\n",
    "                            goblins,\n",
    "                            infinite_purphoros,\n",
    "                            kykar_cards,\n",
    "                            rogues,\n",
    "                            tokens, )\n",
    "\n",
    "    it = [(kykar_cards, ['W', 'U', 'R'], 'kykar.csv'),\n",
    "          (infinite_purphoros, ['R'], 'infinite_purphoros.csv'),\n",
    "          (tokens, ['W', 'G'], 'tokens.csv'),\n",
    "          (goblins, ['B', 'R'], 'goblins.csv'),\n",
    "          (rogues, ['U', 'B', 'R'], 'rogues.csv'),\n",
    "          (esper_blink, ['U', 'B', 'W'], 'esper_blink.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_DECK_RECOMMENDATION_DATASETS:\n",
    "    for (deck_cards, deck_colors, f_out) in it:\n",
    "        print(f\"f_out = {f_out}\")\n",
    "        cards_to_check = (cards\n",
    "                          [cards.setname.isin(sets_to_check)\n",
    "                           & cards.colorIdentity.apply(lambda x: set(x).difference(deck_colors) == set())\n",
    "                           & ~cards.index.isin(deck_cards)]\n",
    "                          .index\n",
    "                          .unique())\n",
    "\n",
    "        df_to_check = pd.DataFrame([{'text_a': card_text.loc[kc, 'text'],\n",
    "                                     'text_b': card_text.loc[ctc, 'text'],\n",
    "                                     'name_a': kc,\n",
    "                                     'name_b': ctc}\n",
    "                                    for kc in deck_cards\n",
    "                                    for ctc in cards_to_check])\n",
    "\n",
    "        print(df_to_check.shape)\n",
    "\n",
    "        df_to_check.to_csv(os.path.join('.', f_out),\n",
    "                           index=False,\n",
    "                           quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now run the following on any machine that has those `csvs` copied to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck_names = ['esper_blink',\n",
    "              'goblins',\n",
    "              'kykar',\n",
    "              'infinite_purphoros',\n",
    "              'rogues',\n",
    "              'tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_DECK_RECOMMENDATIONS:\n",
    "    ds_to_check = (load_dataset('csv',\n",
    "                                data_files={k: f'{k}.csv' for k in deck_names},\n",
    "                                quoting=csv.QUOTE_ALL)\n",
    "                   .map(tokenizer_map_func, batched=True))\n",
    "\n",
    "ds_to_check if DO_DECK_RECOMMENDATIONS else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "version_str = '.v2' if USE_CARD_SAMPLER_V2 else ''\n",
    "\n",
    "if DO_DECK_RECOMMENDATIONS:\n",
    "    for deck_name in deck_names:\n",
    "        print(f\"deck_name = {deck_name}\")\n",
    "        p = trainer.predict(ds_to_check[deck_name])\n",
    "        print(f\"p.predictions.shape = {p.predictions.shape}\")\n",
    "\n",
    "        probs = softmax(p.predictions, axis=1)\n",
    "\n",
    "        z = pd.DataFrame({'p0': probs[:, 0],\n",
    "                          'y_pred': probs.argmax(axis=1),\n",
    "                          'name_b': ds_to_check[deck_name]['name_b'],\n",
    "                          'text_b': ds_to_check[deck_name]['text_b']})\n",
    "        z.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        recs = (z\n",
    "                .groupby(['name_b', 'text_b'])\n",
    "                .p0\n",
    "                .median()\n",
    "                .sort_values(ascending=False)\n",
    "                .reset_index())\n",
    "\n",
    "        recs.to_parquet(f\"{deck_name}{version_str}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now load the `parquet` files created on the other machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if CHECK_DECK_RECOMMENDATIONS:\n",
    "    z = None\n",
    "\n",
    "    for deck_name in deck_names:\n",
    "        z_now = (pd.read_parquet(f\"{deck_name}.parquet\")\n",
    "                 .rename(columns={'p0': f\"p0_{deck_name}\"})\n",
    "                 .drop(columns=['text_b'])\n",
    "                 .set_index('name_b'))\n",
    "        if z is None:\n",
    "            z = z_now\n",
    "        else:\n",
    "            z = z.join(z_now[[f'p0_{deck_name}']],\n",
    "                       how='outer')\n",
    "\n",
    "    z = z.join(card_text)\n",
    "    \n",
    "    p_cols = [f\"p0_{deck_name}\" for deck_name in deck_names]\n",
    "\n",
    "    z.loc[:, 'deckcol'] = z[p_cols].idxmax(axis=1)\n",
    "\n",
    "z.head() if CHECK_DECK_RECOMMENDATIONS else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def foo(deck_name):\n",
    "    z_ip = (z\n",
    "            [z.deckcol == deck_name]\n",
    "            .sort_values(by=deck_name, ascending=False))\n",
    "\n",
    "    z_ip.loc[:, 'over_median'] = z_ip[deck_name] - z_ip[p_cols].min(axis=1)\n",
    "\n",
    "    z_ip.sort_values(by=deck_name, ascending=False, inplace=True)\n",
    "\n",
    "    return z_ip[z_ip.over_median > 0.2].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo('p1_esper_blink') if CHECK_DECK_RECOMMENDATIONS else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo('p1_goblins') if CHECK_DECK_RECOMMENDATIONS else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo('p1_kykar') if CHECK_DECK_RECOMMENDATIONS else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo('p1_infinite_purphoros') if CHECK_DECK_RECOMMENDATIONS else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo('p1_rogues') if CHECK_DECK_RECOMMENDATIONS else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "foo('p1_tokens') if CHECK_DECK_RECOMMENDATIONS else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "def defining_cards(n=5):\n",
    "    top_n_candidates = {deck_col: set(z\n",
    "                                      .sort_values(by=f\"p0_{deck_col}\", ascending=False)\n",
    "                                      .head(n)\n",
    "                                      .index\n",
    "                                      .values)\n",
    "                        for deck_col in deck_names}\n",
    "#     return top_n_candidates\n",
    "#     return collections.Counter({c for c_set in top_n_candidates.values() for c in c_set})\n",
    "    \n",
    "    return {deck_name: (top_n_candidates[deck_name]\n",
    "                        .difference(*[top_n_candidates[dn]\n",
    "                                      for dn in deck_names\n",
    "                                      if dn != deck_name]))\n",
    "            for deck_name in deck_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defining_cards(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bertviz trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# !test -d bertviz_repo && echo \"FYI: bertviz_repo directory already exists, to pull latest version uncomment this line: !rm -r bertviz_repo\"\n",
    "# # !rm -r bertviz_repo # Uncomment if you need a clean pull from repo\n",
    "# !test -d bertviz_repo || git clone https://github.com/jessevig/bertviz bertviz_repo\n",
    "\n",
    "# if not 'bertviz_repo' in sys.path:\n",
    "#     sys.path += ['bertviz_repo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bertviz import head_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%javascript\n",
    "# require.config({\n",
    "#   paths: {\n",
    "#       d3: '//cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min',\n",
    "#     jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
    "#   }\n",
    "# });"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertModel\n",
    "\n",
    "# att_model = BertModel.from_pretrained(model_name, output_attentions=True)\n",
    "\n",
    "# def show_head_view(tokenizer, sentence_a, sentence_b=None):\n",
    "#     inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_special_tokens=True)\n",
    "#     input_ids = inputs['input_ids']\n",
    "#     if sentence_b:\n",
    "#         token_type_ids = inputs['token_type_ids']\n",
    "#         attention = att_model(input_ids, token_type_ids=token_type_ids)[-1]\n",
    "#         sentence_b_start = token_type_ids[0].tolist().index(1)\n",
    "#     else:\n",
    "#         attention = att_model(input_ids)[-1]\n",
    "#         sentence_b_start = None\n",
    "#     input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "#     tokens = tokenizer.convert_ids_to_tokens(input_id_list)    \n",
    "#     head_view(attention, tokens, sentence_b_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_a = card_text.loc['Purphoros, Bronze-Blooded', 'text']\n",
    "# sentence_b = card_text.loc['Hammer of Nazahn', 'text']\n",
    "\n",
    "# show_head_view(tokenizer, sentence_a, sentence_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf bertviz_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# appendix\n",
    "\n",
    "the following is either hacking, didn't work, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizing sentences\n",
    "\n",
    "~~we will be reusing most of the text sentences above several times; might as well tokenize them all up front once instead of tokenizing most of them 100x later~~\n",
    "\n",
    "just do shit the way the documenation suggests we should. do them on the completely built pair parquet files below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaTokenizerFast\n",
    "# tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_tokenizer(row, *args, **kwargs):\n",
    "#     return pd.Series(tokenizer(row.text, *args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (card_text.head(20)\n",
    "#  .apply(my_tokenizer, axis=1, truncation=True, padding=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# card_text = (card_text\n",
    "#              .join(card_text\n",
    "#                    .apply(my_tokenizer, axis=1, truncation=True, padding=True)))\n",
    "\n",
    "# card_text.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### making the pair suggestions dataset\n",
    "\n",
    "okay so we have\n",
    "\n",
    "1. a train / test / val split of all cards\n",
    "1. a series of card text values (our \"sentences\")\n",
    "1. a list of `card --> deck` relationships\n",
    "\n",
    "the task now is to\n",
    "\n",
    "1. generate positive and negative cases for each card\n",
    "    + positive: `card --> deck <-- card`\n",
    "    + negative: just not that\n",
    "1. look up their text values\n",
    "1. write those values to file\n",
    "    + probably want to chunk this up somehow, maybe write 1k sentences per parquet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def card_sampler(cardlist, num_pairs_per_card=10, exclude_cards=None):\n",
    "    exclude_cards = exclude_cards or set()\n",
    "    np.random.seed(1337)\n",
    "    neg_card_options = np.array(list(all_cards.difference(exclude_cards)))\n",
    "    for card in tqdm(cardlist):\n",
    "        rec = card_text.loc[card]\n",
    "        commanders = edhrec_cards[edhrec_cards.name == card].commander.unique()\n",
    "        \n",
    "        # get positive card labels\n",
    "        pos_options = (edhrec_cards[edhrec_cards.commander.isin(commanders)\n",
    "                                    & (edhrec_cards.name != card)\n",
    "                                    & (~edhrec_cards.name.isin(exclude_cards))]\n",
    "                       .name.unique())\n",
    "\n",
    "        n_labels = min(num_pairs_per_card, pos_options.shape[0])\n",
    "        pos_cards = np.random.choice(pos_options,\n",
    "                                     size=n_labels,\n",
    "                                     replace=False)\n",
    "        \n",
    "        for pos_card in pos_cards:\n",
    "            try:\n",
    "                pos_text_rec = card_text.loc[pos_card]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    pos_text_rec = card_text.loc[pos_card.replace('//', '/')]\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            yield (*rec, *pos_text_rec, True)\n",
    "        \n",
    "        # get negative samples\n",
    "        # start by getting 3x as many as we had before, and assume that this\n",
    "        # will net at least x that weren't options before.\n",
    "        # additionally, we are comfortable having a full set of negative labels\n",
    "        # even if we had few (or no!) positive labels\n",
    "        neg_cards = np.random.choice(neg_card_options,\n",
    "                                     size=3 * num_pairs_per_card,\n",
    "                                     replace=False)\n",
    "        \n",
    "        neg_cards = list(set(neg_cards).difference(pos_cards))[:n_labels]\n",
    "        for neg_card in neg_cards:\n",
    "            neg_text_rec = card_text.loc[neg_card]\n",
    "            yield (*rec, *neg_text_rec, False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!rm -r ./data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# import os\n",
    "\n",
    "# for split_type in ['train', 'test', 'val']:\n",
    "#     for label_type in ['pos', 'neg']:\n",
    "#         os.makedirs(os.path.join('.', 'data', split_type, label_type), exist_ok=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "for split_type in ['train', 'test', 'val']:\n",
    "    os.makedirs(os.path.join('.', 'data', split_type), exist_ok=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!tree ."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import itertools\n",
    "\n",
    "def grouper(iterable, n, fillvalue=None):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return itertools.zip_longest(*args, fillvalue=fillvalue)\n",
    "\n",
    "# for g in grouper(range(20), 3):\n",
    "#     print(type(g))\n",
    "#     print(g)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# for g in grouper(card_sampler(cards_val[:3], 10), 4):\n",
    "#     for rec in g:\n",
    "#         print(rec)\n",
    "#     print()\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "NUM_PAIRS_PER_CARD = 100\n",
    "PARQUET_CHUNK_SIZE = 100_000\n",
    "\n",
    "for (split_type, cardlist, exclude_cardlist) in [['val', cards_val, None],\n",
    "                                                 ['test', cards_test, None],\n",
    "                                                 ['train', cards_train, set(cards_val).union(cards_test)], ]:\n",
    "    root = os.path.join('.', split_type)\n",
    "    sampler = card_sampler(cardlist, NUM_PAIRS_PER_CARD, exclude_cardlist)\n",
    "    for (i, grp) in enumerate(tqdm(grouper(sampler, PARQUET_CHUNK_SIZE))):\n",
    "        (pd.DataFrame(grp, columns=['text_a', 'text_b', 'label'])\n",
    "         .dropna()\n",
    "         .to_parquet(os.path.join('.', 'data', split_type, f'part-{i:0>5}.parquet')))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!tree ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the pytorch datasets\n",
    "\n",
    "basing this in large part off of [this doc page](https://huggingface.co/transformers/custom_datasets.html#nlplib)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_parquet('./data/train')\n",
    "test = pd.read_parquet('./data/test')\n",
    "val = pd.read_parquet('./data/val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### do the encodings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_encodings = tokenizer(test.text_a.values.tolist(),\n",
    "                           test.text_b.values.tolist(),\n",
    "                           truncation=True,\n",
    "                           padding=True)\n",
    "\n",
    "with open('test_encodings.pkl', 'wb') as fp:\n",
    "    pickle.dump(test_encodings, fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('test_encodings.pkl', 'rb') as fp:\n",
    "    test_encodings = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val_encodings = tokenizer(val.text_a.values.tolist(),\n",
    "                          val.text_b.values.tolist(),\n",
    "                          truncation=True,\n",
    "                          padding=True)\n",
    "\n",
    "with open('val_encodings.pkl', 'wb') as fp:\n",
    "    pickle.dump(test_encodings, fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('val_encodings.pkl', 'rb') as fp:\n",
    "    val_encodings = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[len(_) for _ in val_encodings['input_ids'][:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, the below killed the kernel... :("
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "train_encodings = tokenizer(train.text_a.values.tolist(),\n",
    "                            train.text_b.values.tolist(),\n",
    "                            truncation=True,\n",
    "                            padding=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "with open('train_encodings.pkl', 'wb') as fp:\n",
    "    pickle.dump(train_encodings, fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "with open('train_encodings.pkl', 'rb') as fp:\n",
    "    train_encodings = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "idx = 0\n",
    "rec = train.iloc[idx]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "class EdhrecDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
