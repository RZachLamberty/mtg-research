{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDH card pair prediction\n",
    "\n",
    "1. build train / test / dev data set\n",
    "    1. get EDH card pair recommendations from edhrec.com. these have prediction value 1\n",
    "    1. generate false pairs (prediction value 0) by randomly generating pairs\n",
    "    1. split, stratifying on card color identity, card type, rarity.\n",
    "    1. convert cards into sentences\n",
    "1. fine-tune\n",
    "    1. load pre-trained bert model on prediction task \"card a, card b --> {yes,no} was edh rec\n",
    "1. make deck predictions for one of my existing decks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (BertConfig, BertTokenizerFast,\n",
    "                          BertForNextSentencePrediction,\n",
    "                          DataCollatorWithPadding,\n",
    "                          PreTrainedModel, PreTrainedTokenizerFast,\n",
    "                          Trainer, TrainingArguments, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build train / test / dev data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get EDH card pair recommendations from edhrec.com\n",
    "\n",
    "these will have prediction value 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import mtg.cards\n",
    "import mtg.extract.edhrec"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "edhrec_cards = (mtg.extract.edhrec.get_commanders_and_cards()\n",
    "                [['name', 'commander']])\n",
    "edhrec_cards.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# most common cards\n",
    "edhrec_cards.name.value_counts().plot.hist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# most common cards\n",
    "vc = edhrec_cards.name.value_counts()\n",
    "vc.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# most cards appear in only 1 commander recs, up to 500 cards appear\n",
    "# in 10 commander recs\n",
    "vc[vc <= 10].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we just ran with this, how many total pairs could we generate this way? basically, for every card in deck X, every other card is a valid pair. that's:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "z = edhrec_cards.commander.value_counts()\n",
    "f\"{int((z * (z - 1) / 2).sum()):,}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at first I was going to say no way, buuuuuut it's actually not terrible... we want big data, after all\n",
    "\n",
    "we would need to generate about 32 min negative labels if that were the dataset we were interested in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get all cards from mtgjson\n",
    "\n",
    "to generate false pairs we will randomly select from all cards. about 65% of all MTG cards are referenced on edhrec, but the rest are also, presumably, good choices for 0 labels"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cards = (mtg.cards.cards_df()\n",
    "         .sort_values(by=['name', 'multiverseId'], ascending=False)\n",
    "         .groupby('name')\n",
    "         .first())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# before groupby().last(): 56_002, 78\n",
    "# after: 21_814, 77\n",
    "cards.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_cards = set(cards.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can eventually use this dataframe to create a generator of true card pairs off of a single card anchor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split, stratifying on card color identity, card type, rarity.\n",
    "\n",
    "we will split on cards. this is actually tricky, right? it would be easy if we could just do a 95/5/5 and then there was enough pairing between 5s and other 5s to build an entire test / val set, but I actually suspect we might have a problem fielding that many extra records. oh well, I guess we'll tell in due time\n",
    "\n",
    "since we want to stratify on so many things, and we have a 2/3s chance of any card being in the true label, I actually think fully random sampling is approporiate. we can look at the breakdown of that by other features if we need to"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cards_train, cards_test_val = train_test_split(cards.index.values, test_size=.1, random_state=1337)\n",
    "cards_test, cards_val = train_test_split(cards_test_val, test_size=.5, random_state=1337)\n",
    "\n",
    "print(f\"\"\"\n",
    "train: {cards_train.shape[0]}\n",
    "test: {cards_test.shape[0]}\n",
    "val: {cards_val.shape[0]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert cards into sentences"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cmc_map = {0.0: 'zero',\n",
    "           0.5: 'one half',\n",
    "           1.0: 'one',\n",
    "           2.0: 'two',\n",
    "           3.0: 'three',\n",
    "           4.0: 'four',\n",
    "           5.0: 'five',\n",
    "           6.0: 'six',\n",
    "           7.0: 'seven',\n",
    "           8.0: 'eight',\n",
    "           9.0: 'nine',\n",
    "           10.0: 'ten',\n",
    "           11.0: 'eleven',\n",
    "           12.0: 'twelve',\n",
    "           13.0: 'thirteen',\n",
    "           14.0: 'fourteen',\n",
    "           15.0: 'fifteen',\n",
    "           16.0: 'sixteen',\n",
    "           1000000.0: 'one million', }\n",
    "\n",
    "\n",
    "color_map = {'W': 'white', 'U': 'blue', 'B': 'black', 'R': 'red', 'G': 'green'}\n",
    "\n",
    "\n",
    "def parse_mana_colors_from_cost(mc):\n",
    "    return ', '.join(color_map[c] for c in 'WUBRG' if c in (mc or ''))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "assert parse_mana_colors_from_cost('{2}{U}{U}{B}') == 'blue, black'\n",
    "assert parse_mana_colors_from_cost('{8}{W}{W}') == 'white'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_card_text(card):\n",
    "    mana_color_str = parse_mana_colors_from_cost(card.manaCost)\n",
    "    cmc_str = f\"{cmc_map[card.convertedManaCost]} mana\"\n",
    "    \n",
    "    if mana_color_str != '':\n",
    "        mana_color_str = f' including {mana_color_str}'\n",
    "    \n",
    "    return (f\"for {cmc_str}{mana_color_str}, cast {card.type} {card.name}: {card.text}\"\n",
    "            .lower()\n",
    "            .replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "name = cards_train[4]\n",
    "cards.loc[name]\n",
    "get_card_text(cards.loc[name])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "card_text = pd.DataFrame({'text': cards.apply(get_card_text, axis=1)})\n",
    "card_text.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's just go with this, see how it works out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a `huggingface` `datasets`\n",
    "\n",
    "following along with the relatively simple example [here](https://github.com/huggingface/datasets/blob/master/datasets/squad/squad.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### custom dataset loader?\n",
    "\n",
    "meh let's try the `csv` loader first"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class Edhrec(datasets.GeneratorBasedBuilder):\n",
    "    raise NotImplementedError(\"havent written builder configs\")\n",
    "    BUILDER_CONFIGS = []\n",
    "    \n",
    "    def _info(self):\n",
    "        return datasets.DatasetInfo(\n",
    "            description=\"lol no thanks\",\n",
    "            features=datasets.Features({\"id\": datasets.Value('string'),\n",
    "                                        \"text_a\": datasets.Value('string'),\n",
    "                                        \"text_b\": datasets.Value('string'),\n",
    "                                        \"label\": datasets.Value(\"int32\"), }),\n",
    "            supervised_keys=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `csv` loader\n",
    "\n",
    "generate `csv`s the same way we were doing `parquet` (see appendix) and load those as datasets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def card_sampler(cardlist, num_pairs_per_card=10, exclude_cards=None):\n",
    "    exclude_cards = exclude_cards or set()\n",
    "    np.random.seed(1337)\n",
    "    neg_card_options = np.array(list(all_cards.difference(exclude_cards)))\n",
    "    for card in tqdm(cardlist):\n",
    "        rec = card_text.loc[card]\n",
    "        commanders = edhrec_cards[edhrec_cards.name == card].commander.unique()\n",
    "        \n",
    "        # get positive card labels\n",
    "        pos_options = (edhrec_cards[edhrec_cards.commander.isin(commanders)\n",
    "                                    & (edhrec_cards.name != card)\n",
    "                                    & (~edhrec_cards.name.isin(exclude_cards))]\n",
    "                       .name.unique())\n",
    "\n",
    "        n_labels = min(num_pairs_per_card, pos_options.shape[0])\n",
    "        pos_cards = np.random.choice(pos_options,\n",
    "                                     size=n_labels,\n",
    "                                     replace=False)\n",
    "        \n",
    "        for pos_card in pos_cards:\n",
    "            try:\n",
    "                pos_text_rec = card_text.loc[pos_card]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    pos_text_rec = card_text.loc[pos_card.replace('//', '/')]\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            yield (*rec, *pos_text_rec, True)\n",
    "        \n",
    "        # get negative samples:\n",
    "        # start by getting 3x as many as we had before, and assume that this\n",
    "        # will net at least x that aren't positive label options. additionally,\n",
    "        # we are comfortable having a full set of negative labels even if we\n",
    "        # had few (or no!) positive labels\n",
    "        neg_cards = np.random.choice(neg_card_options,\n",
    "                                     size=3 * num_pairs_per_card,\n",
    "                                     replace=False)\n",
    "        \n",
    "        neg_cards = list(set(neg_cards).difference(pos_cards))[:n_labels]\n",
    "        for neg_card in neg_cards:\n",
    "            neg_text_rec = card_text.loc[neg_card]\n",
    "            yield (*rec, *neg_text_rec, False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def grouper(iterable, n, fillvalue=None):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return itertools.zip_longest(*args, fillvalue=fillvalue)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!rm -r ./data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for split_type in ['train', 'test', 'val']:\n",
    "    os.makedirs(os.path.join('.', 'data', split_type), exist_ok=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!tree ."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NUM_PAIRS_PER_CARD = 100\n",
    "PARQUET_CHUNK_SIZE = 100_000\n",
    "\n",
    "for (split_type, cardlist, exclude_cardlist) in [['val', cards_val, None],\n",
    "                                                 ['test', cards_test, None],\n",
    "                                                 ['train', cards_train, set(cards_val).union(cards_test)], ]:\n",
    "    root = os.path.join('.', split_type)\n",
    "    sampler = card_sampler(cardlist, NUM_PAIRS_PER_CARD, exclude_cardlist)\n",
    "    for (i, grp) in enumerate(tqdm(grouper(sampler, PARQUET_CHUNK_SIZE))):\n",
    "        (pd.DataFrame(grp, columns=['text_a', 'text_b', 'label'])\n",
    "         .dropna()\n",
    "         .to_csv(os.path.join('.', 'data', split_type, f'part-{i:0>5}.csv'),\n",
    "                 index=False,\n",
    "                 quoting=csv.QUOTE_ALL))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!tree ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading csvs, shuffling, tokenizing, etc datasets now\n",
    "\n",
    "+ tokenizing from [here](https://huggingface.co/docs/datasets/processing.html#processing-data-in-batches)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# F_DS_CACHE = os.path.join('.', 'data', 'edhrec_cache_dataset')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MAX_LENGTH = 300\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenizer_map_func(rec):\n",
    "    return tokenizer(rec['text_a'], rec['text_b'],\n",
    "                     padding='max_length',\n",
    "                     max_length=MAX_LENGTH,\n",
    "                     truncation=True)\n",
    "\n",
    "\n",
    "def fix_label(rec):\n",
    "    return {'label_as_int': [int(_) for _ in rec['label']]}\n",
    "\n",
    "\n",
    "split_types = ['val', 'test', 'train']\n",
    "dataset = (load_dataset('csv',\n",
    "                        data_files={split_type: glob(os.path.join('.', 'data', split_type, '*.csv'))\n",
    "                                    for split_type in split_types},\n",
    "                        quoting=csv.QUOTE_ALL)\n",
    "           #.map(fix_label,\n",
    "           #     batched=True)\n",
    "           .shuffle(seeds={split_type: 1337\n",
    "                           for split_type in split_types})\n",
    "           .map(tokenizer_map_func,\n",
    "                batched=True))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# dataset = load_from_disk(F_DS_CACHE)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# dataset['val'][0]['label_as_int']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine-tune"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "\n",
    "TRAIN_BATCH = 36\n",
    "EVAL_BATCH = 36\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                   # output directory\n",
    "    num_train_epochs=1,                       # total # of training epochs\n",
    "    per_device_train_batch_size=TRAIN_BATCH,  # batch size per device during training\n",
    "    per_device_eval_batch_size=EVAL_BATCH,    # batch size for evaluation\n",
    "    warmup_steps=500,                         # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                        # strength of weight decay\n",
    "    logging_dir='./logs',                     # directory for storing logs\n",
    "    # my custom ones\n",
    "    logging_steps=250,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy='steps',\n",
    "    logging_first_step=True,\n",
    "    seed=1337,\n",
    "    dataloader_drop_last=True,\n",
    "    dataloader_num_workers=30,\n",
    "    label_names=['labels'],\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=10,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                              # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                       # training arguments, defined above\n",
    "    train_dataset=dataset['train'],           # training dataset\n",
    "    eval_dataset=dataset['val'].select(range(EVAL_BATCH * 1_000)),  # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_path = \"edhrec-bert-base-uncased\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainer.evaluate(dataset['val'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainer.evaluate(dataset['test'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainer.predict?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "p = trainer.predict(dataset['test'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset['test']['text_a'][0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset['test']['text_b'][0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "zero_max = p.predictions[:, 0].argmax()\n",
    "p.predictions[zero_max]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset['test']['text_a'][zero_max]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset['test']['text_b'][zero_max]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "one_max = p.predictions[:, 1].argmax()\n",
    "p.predictions[one_max]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset['test']['text_a'][one_max]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset['test']['text_b'][one_max]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{k: np.array(v) for (k, v) in dataset['test'][:10].items() if k in ['attention_mask', 'input_ids', 'label', 'token_type_ids']}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "(model(**{k: torch.as_tensor(np.array(v)).to(\"cuda\")\n",
    "          for (k, v) in dataset['test'][:10].items()\n",
    "          if k in ['attention_mask', 'input_ids', 'token_type_ids']})\n",
    " [0]\n",
    " .softmax(1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset['test']['label'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## double-checking our trained model\n",
    "\n",
    "next steps\n",
    "\n",
    "+ what do our false positives look like\n",
    "+ what is the separation like for \"cards that have been on edhrec\" vs. \"cards that havent\n",
    "    + i.e. do we just predict \"both cards have been on EDHREC\"?\n",
    "    + did we create a dataset that is just (edhrec cards, either type)? I thought we were making (either type, either type)\n",
    "+ what is the sorted list of recommendations given an existing deck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why are these all only edhrec cards? I thought I was generating pairs from both sides?\n",
    "\n",
    "is this a problem? when a new card shows up and has never been seen before, will the model be unable to handle it? I think not, because presumably there were cards in test / val that it had never seen before (have I verified that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the trained model\n",
    "config = BertConfig.from_pretrained('edhrec-bert-base-uncased')\n",
    "model = BertForNextSentencePrediction.from_pretrained('edhrec-bert-base-uncased', config=config)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('edhrec-bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_BATCH = 36\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./ignore',\n",
    "    per_device_eval_batch_size=EVAL_BATCH,    # batch size for evaluation\n",
    "    label_names=['labels'],\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 300\n",
    "\n",
    "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenizer_map_func(rec):\n",
    "    return tokenizer(rec['text_a'], rec['text_b'],\n",
    "                     padding='max_length',\n",
    "                     max_length=MAX_LENGTH,\n",
    "                     truncation=True)\n",
    "\n",
    "\n",
    "def fix_label(rec):\n",
    "    return {'label_as_int': [int(_) for _ in rec['label']]}\n",
    "\n",
    "\n",
    "split_types = ['val', 'test', 'train']\n",
    "dataset = (load_dataset('csv',\n",
    "                        data_files={split_type: sorted(glob(os.path.join('.', 'data', split_type, '*.csv')))\n",
    "                                    for split_type in split_types},\n",
    "                        quoting=csv.QUOTE_ALL)\n",
    "           #.map(fix_label,\n",
    "           #     batched=True)\n",
    "           .shuffle(seeds={split_type: 1337\n",
    "                           for split_type in split_types})\n",
    "           .map(tokenizer_map_func,\n",
    "                batched=True))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p = trainer.predict(dataset['test'].select(range(1_000)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p.predictions.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p.label_ids.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p.metrics"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "probs = softmax(p.predictions, axis=1)\n",
    "\n",
    "z = pd.DataFrame({'p1': probs[:, 1],\n",
    "                  'y_pred': probs.argmax(axis=1),\n",
    "                  'y': p.label_ids})\n",
    "z.loc[:, 'is_right'] = z.y == z.y_pred\n",
    "\n",
    "z.sort_values(by='p1', ascending=False, inplace=True)\n",
    "z.reset_index(drop=True, inplace=True)\n",
    "\n",
    "z.is_right.value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "total_true = z.y.sum()\n",
    "\n",
    "(z.y.cumsum() / total_true).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "+ make the combo dataframe\n",
    "+ convert that into a dataset (probably a `.from_pandas` or some shit)\n",
    "+ pass that to eval\n",
    "+ sort by predictions\n",
    "+ profit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!ls -alh ../../mtg/mtg/tags/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!touch ~/.virtualenvs/mtg-research/src/mtg/mtg/tags.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the following on a computer with `mtg` installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kykar_cards = [\n",
    "    \"Aetherflux Reservoir\",\n",
    "    \"Anointed Procession\",\n",
    "    \"As Foretold\",\n",
    "    \"Austere Command\",\n",
    "    \"Baral, Chief of Compliance\",\n",
    "    \"Blue Sun's Zenith\",\n",
    "    \"Boros Charm\",\n",
    "    \"Chaos Warp\",\n",
    "    \"Counterspell\",\n",
    "    \"Cultivator's Caravan\",\n",
    "    \"Cyclonic Rift\",\n",
    "    \"Desolate Lighthouse\",\n",
    "    \"Disallow\",\n",
    "    \"Dismantling Blow\",\n",
    "    \"Docent of Perfection\",\n",
    "    \"Dovin's Veto\",\n",
    "    \"Eerie Interlude\",\n",
    "    \"Fact or Fiction\",\n",
    "    \"Faithless Looting\",\n",
    "    \"Gitaxian Probe\",\n",
    "    \"Guttersnipe\",\n",
    "    \"Impulse\",\n",
    "    \"Kykar, Wind's Fury\",\n",
    "    \"Mentor of the Meek\",\n",
    "    \"Mizzix of the Izmagnus\",\n",
    "    \"Mizzix's Mastery\",\n",
    "    \"Murmuring Mystic\",\n",
    "    \"Mystic Confluence\",\n",
    "    \"Mystic Speculation\",\n",
    "    \"Mystical Tutor\",\n",
    "    \"Narset Transcendent\",\n",
    "    \"Neurok Stealthsuit\",\n",
    "    \"Niv-Mizzet, Parun\",\n",
    "    \"Omniscience\",\n",
    "    \"Ponder\",\n",
    "    \"Preordain\",\n",
    "    \"Primal Amulet\",\n",
    "    \"Ral, Izzet Viceroy\",\n",
    "    \"Reliquary Tower\",\n",
    "    \"Render Silent\",\n",
    "    \"Rhystic Study\",\n",
    "    \"Serum Visions\",\n",
    "    \"Sram's Expertise\",\n",
    "    \"Stroke of Genius\",\n",
    "    \"Sunforger\",\n",
    "    \"Supreme Verdict\",\n",
    "    \"Swords to Plowshares\",\n",
    "    \"Taigam, Ojutai Master\",\n",
    "    \"Talrand, Sky Summoner\",\n",
    "    \"Teferi, Hero of Dominaria\",\n",
    "    \"Teferi, Time Raveler\",\n",
    "    \"The Locust God\",\n",
    "    \"Thought Vessel\",\n",
    "    \"Tidespout Tyrant\",\n",
    "    \"Trail of Evidence\",\n",
    "    \"Vandalblast\",\n",
    "    \"Young Pyromancer\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_to_check = [\n",
    "    \"2XM\",\n",
    "    \"AKR\",\n",
    "    \"C20\",\n",
    "    \"CC1\",\n",
    "    \"CMC\",\n",
    "    \"CMR\",\n",
    "    \"IKO\",\n",
    "    \"JMP\",\n",
    "    \"KHC\",\n",
    "    \"M21\",\n",
    "    \"MB1\",\n",
    "    \"MH2\",\n",
    "    \"Q03\",\n",
    "    \"SLD\",\n",
    "    \"SLU\",\n",
    "    \"SS3\",\n",
    "    \"THB\",\n",
    "    \"TSR\",\n",
    "    \"ZNC\",\n",
    "    \"ZNE\",\n",
    "    \"ZNR\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_to_check = (cards\n",
    "                  [cards.setname.isin(sets_to_check)\n",
    "                   & (~cards.rarity.isin(['common', 'uncommon']))\n",
    "                   & cards.colorIdentity.apply(lambda x: set(x).difference(['W', 'U', 'R']) == set())\n",
    "                   & ~cards.index.isin(kykar_cards)]\n",
    "                  .index\n",
    "                  .unique())\n",
    "cards_to_check.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_text.loc[kykar_cards[0], 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_check = pd.DataFrame([{'text_a': card_text.loc[kc, 'text'],\n",
    "                             'text_b': card_text.loc[ctc, 'text'],\n",
    "                             'name_a': kc,\n",
    "                             'name_b': ctc}\n",
    "                            for kc in kykar_cards\n",
    "                            for ctc in cards_to_check])\n",
    "\n",
    "df_to_check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_check.to_csv(os.path.join('.', 'kykar.csv'),\n",
    "                   index=False,\n",
    "                   quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now run the following on any machine that has `kykar.csv` copied to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_to_check = (load_dataset('csv',\n",
    "                            data_files='kykar.csv',\n",
    "                            quoting=csv.QUOTE_ALL)\n",
    "               .map(tokenizer_map_func, batched=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = trainer.predict(ds_to_check)\n",
    "\n",
    "p.predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "probs = softmax(p.predictions, axis=1)\n",
    "\n",
    "z = pd.DataFrame({'p1': probs[:, 1],\n",
    "                  'y_pred': probs.argmax(axis=1),\n",
    "                  'y': p.label_ids})\n",
    "z.loc[:, 'is_right'] = z.y == z.y_pred\n",
    "\n",
    "z.sort_values(by='p1', ascending=False, inplace=True)\n",
    "z.reset_index(drop=True, inplace=True)\n",
    "\n",
    "z.is_right.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_true = z.y.sum()\n",
    "\n",
    "(z.y.cumsum() / total_true).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1k = dataset['test'].select(range(1_000))\n",
    "d1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1h = dataset['test'].select(range(100))\n",
    "d1h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_preds(n=100):\n",
    "    chunk_size = 100\n",
    "    z = None\n",
    "    \n",
    "    i = 0\n",
    "    while i < n:\n",
    "        print(f\"i = {i}\")\n",
    "        d = dataset['test'][i: i + chunk_size]\n",
    "        p = (model(**{k: torch.as_tensor(np.array(v))\n",
    "                  for (k, v) in d.items()\n",
    "                  if k in ['attention_mask', 'input_ids', 'token_type_ids']})\n",
    "         [0]\n",
    "         .softmax(1))\n",
    "\n",
    "        z_now = pd.DataFrame(p.detach().numpy(), columns=['p0', 'p1'])\n",
    "        for key in ['label', 'text_a', 'text_b']:\n",
    "            z_now.loc[:, key] = d[key]\n",
    "        \n",
    "        if z is None:\n",
    "            z = z_now\n",
    "        else:\n",
    "            z = z.append(z_now, ignore_index=True)\n",
    "        \n",
    "        i += chunk_size\n",
    "        \n",
    "    z.reset_index(drop=True, inplace=True)\n",
    "    z.loc[:, 'p_delta'] = (z.p0 - z.p1).abs()\n",
    "    \n",
    "    z.loc[:, 'is_right'] = (z.p1 > z.p0) == z.label\n",
    "\n",
    "    z.sort_values(by='p_delta', inplace=True, ascending=False)\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = get_preds(500)\n",
    "\n",
    "z.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.is_right.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[~z.is_right].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_edhrec = card_text.copy()\n",
    "is_edhrec.loc[:, 'is_edhrec'] = is_edhrec.index.isin(edhrec_cards.name.unique())\n",
    "is_edhrec.reset_index(inplace=True)\n",
    "is_edhrec.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(z\n",
    " .merge(is_edhrec.rename(columns={'text': 'text_a', 'is_edhrec': 'is_edhrec_a'})[['text_a', 'is_edhrec_a']],\n",
    "        how='left',\n",
    "        on='text_a')\n",
    " .merge(is_edhrec.rename(columns={'text': 'text_b', 'is_edhrec': 'is_edhrec_b'})[['text_b', 'is_edhrec_b']],\n",
    "        how='left',\n",
    "        on='text_b')\n",
    " .groupby(['is_right', 'is_edhrec_b'])\n",
    " .is_right.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make deck predictions for one of my existing decks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# appendix\n",
    "\n",
    "the following is either hacking, didn't work, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizing sentences\n",
    "\n",
    "~~we will be reusing most of the text sentences above several times; might as well tokenize them all up front once instead of tokenizing most of them 100x later~~\n",
    "\n",
    "just do shit the way the documenation suggests we should. do them on the completely built pair parquet files below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaTokenizerFast\n",
    "# tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_tokenizer(row, *args, **kwargs):\n",
    "#     return pd.Series(tokenizer(row.text, *args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (card_text.head(20)\n",
    "#  .apply(my_tokenizer, axis=1, truncation=True, padding=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# card_text = (card_text\n",
    "#              .join(card_text\n",
    "#                    .apply(my_tokenizer, axis=1, truncation=True, padding=True)))\n",
    "\n",
    "# card_text.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### making the pair suggestions dataset\n",
    "\n",
    "okay so we have\n",
    "\n",
    "1. a train / test / val split of all cards\n",
    "1. a series of card text values (our \"sentences\")\n",
    "1. a list of `card --> deck` relationships\n",
    "\n",
    "the task now is to\n",
    "\n",
    "1. generate positive and negative cases for each card\n",
    "    + positive: `card --> deck <-- card`\n",
    "    + negative: just not that\n",
    "1. look up their text values\n",
    "1. write those values to file\n",
    "    + probably want to chunk this up somehow, maybe write 1k sentences per parquet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def card_sampler(cardlist, num_pairs_per_card=10, exclude_cards=None):\n",
    "    exclude_cards = exclude_cards or set()\n",
    "    np.random.seed(1337)\n",
    "    neg_card_options = np.array(list(all_cards.difference(exclude_cards)))\n",
    "    for card in tqdm(cardlist):\n",
    "        rec = card_text.loc[card]\n",
    "        commanders = edhrec_cards[edhrec_cards.name == card].commander.unique()\n",
    "        \n",
    "        # get positive card labels\n",
    "        pos_options = (edhrec_cards[edhrec_cards.commander.isin(commanders)\n",
    "                                    & (edhrec_cards.name != card)\n",
    "                                    & (~edhrec_cards.name.isin(exclude_cards))]\n",
    "                       .name.unique())\n",
    "\n",
    "        n_labels = min(num_pairs_per_card, pos_options.shape[0])\n",
    "        pos_cards = np.random.choice(pos_options,\n",
    "                                     size=n_labels,\n",
    "                                     replace=False)\n",
    "        \n",
    "        for pos_card in pos_cards:\n",
    "            try:\n",
    "                pos_text_rec = card_text.loc[pos_card]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    pos_text_rec = card_text.loc[pos_card.replace('//', '/')]\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            yield (*rec, *pos_text_rec, True)\n",
    "        \n",
    "        # get negative samples\n",
    "        # start by getting 3x as many as we had before, and assume that this\n",
    "        # will net at least x that weren't options before.\n",
    "        # additionally, we are comfortable having a full set of negative labels\n",
    "        # even if we had few (or no!) positive labels\n",
    "        neg_cards = np.random.choice(neg_card_options,\n",
    "                                     size=3 * num_pairs_per_card,\n",
    "                                     replace=False)\n",
    "        \n",
    "        neg_cards = list(set(neg_cards).difference(pos_cards))[:n_labels]\n",
    "        for neg_card in neg_cards:\n",
    "            neg_text_rec = card_text.loc[neg_card]\n",
    "            yield (*rec, *neg_text_rec, False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!rm -r ./data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# import os\n",
    "\n",
    "# for split_type in ['train', 'test', 'val']:\n",
    "#     for label_type in ['pos', 'neg']:\n",
    "#         os.makedirs(os.path.join('.', 'data', split_type, label_type), exist_ok=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "for split_type in ['train', 'test', 'val']:\n",
    "    os.makedirs(os.path.join('.', 'data', split_type), exist_ok=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!tree ."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import itertools\n",
    "\n",
    "def grouper(iterable, n, fillvalue=None):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return itertools.zip_longest(*args, fillvalue=fillvalue)\n",
    "\n",
    "# for g in grouper(range(20), 3):\n",
    "#     print(type(g))\n",
    "#     print(g)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# for g in grouper(card_sampler(cards_val[:3], 10), 4):\n",
    "#     for rec in g:\n",
    "#         print(rec)\n",
    "#     print()\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "NUM_PAIRS_PER_CARD = 100\n",
    "PARQUET_CHUNK_SIZE = 100_000\n",
    "\n",
    "for (split_type, cardlist, exclude_cardlist) in [['val', cards_val, None],\n",
    "                                                 ['test', cards_test, None],\n",
    "                                                 ['train', cards_train, set(cards_val).union(cards_test)], ]:\n",
    "    root = os.path.join('.', split_type)\n",
    "    sampler = card_sampler(cardlist, NUM_PAIRS_PER_CARD, exclude_cardlist)\n",
    "    for (i, grp) in enumerate(tqdm(grouper(sampler, PARQUET_CHUNK_SIZE))):\n",
    "        (pd.DataFrame(grp, columns=['text_a', 'text_b', 'label'])\n",
    "         .dropna()\n",
    "         .to_parquet(os.path.join('.', 'data', split_type, f'part-{i:0>5}.parquet')))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!tree ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the pytorch datasets\n",
    "\n",
    "basing this in large part off of [this doc page](https://huggingface.co/transformers/custom_datasets.html#nlplib)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_parquet('./data/train')\n",
    "test = pd.read_parquet('./data/test')\n",
    "val = pd.read_parquet('./data/val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### do the encodings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_encodings = tokenizer(test.text_a.values.tolist(),\n",
    "                           test.text_b.values.tolist(),\n",
    "                           truncation=True,\n",
    "                           padding=True)\n",
    "\n",
    "with open('test_encodings.pkl', 'wb') as fp:\n",
    "    pickle.dump(test_encodings, fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('test_encodings.pkl', 'rb') as fp:\n",
    "    test_encodings = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val_encodings = tokenizer(val.text_a.values.tolist(),\n",
    "                          val.text_b.values.tolist(),\n",
    "                          truncation=True,\n",
    "                          padding=True)\n",
    "\n",
    "with open('val_encodings.pkl', 'wb') as fp:\n",
    "    pickle.dump(test_encodings, fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('val_encodings.pkl', 'rb') as fp:\n",
    "    val_encodings = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[len(_) for _ in val_encodings['input_ids'][:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, the below killed the kernel... :("
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "train_encodings = tokenizer(train.text_a.values.tolist(),\n",
    "                            train.text_b.values.tolist(),\n",
    "                            truncation=True,\n",
    "                            padding=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "with open('train_encodings.pkl', 'wb') as fp:\n",
    "    pickle.dump(train_encodings, fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "with open('train_encodings.pkl', 'rb') as fp:\n",
    "    train_encodings = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "idx = 0\n",
    "rec = train.iloc[idx]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "class EdhrecDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
