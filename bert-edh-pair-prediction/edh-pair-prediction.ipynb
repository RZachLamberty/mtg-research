{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDH card pair prediction\n",
    "\n",
    "1. build train / test / dev data set\n",
    "    1. get EDH card pair recommendations from edhrec.com. these have prediction value 1\n",
    "    1. generate false pairs (prediction value 0) by randomly generating pairs\n",
    "    1. split, stratifying on card color identity, card type, rarity.\n",
    "    1. convert cards into sentences\n",
    "1. fine-tune\n",
    "    1. load pre-trained bert model on prediction task \"card a, card b --> {yes,no} was edh rec\n",
    "1. make deck predictions for one of my existing decks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (BertTokenizerFast,\n",
    "                          BertForNextSentencePrediction,\n",
    "                          Trainer, TrainingArguments, )\n",
    "\n",
    "import mtg.cards\n",
    "import mtg.extract.edhrec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build train / test / dev data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get EDH card pair recommendations from edhrec.com\n",
    "\n",
    "these will have prediction value 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edhrec_cards = (mtg.extract.edhrec.get_commanders_and_cards()\n",
    "                [['name', 'commander']])\n",
    "edhrec_cards.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common cards\n",
    "edhrec_cards.name.value_counts().plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common cards\n",
    "vc = edhrec_cards.name.value_counts()\n",
    "vc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most cards appear in only 1 commander recs, up to 500 cards appear\n",
    "# in 10 commander recs\n",
    "vc[vc <= 10].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we just ran with this, how many total pairs could we generate this way? basically, for every card in deck X, every other card is a valid pair. that's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = edhrec_cards.commander.value_counts()\n",
    "f\"{int((z * (z - 1) / 2).sum()):,}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at first I was going to say no way, buuuuuut it's actually not terrible... we want big data, after all\n",
    "\n",
    "we would need to generate about 32 min negative labels if that were the dataset we were interested in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get all cards from mtgjson\n",
    "\n",
    "to generate false pairs we will randomly select from all cards. about 65% of all MTG cards are referenced on edhrec, but the rest are also, presumably, good choices for 0 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards = (mtg.cards.cards_df()\n",
    "         .sort_values(by=['name', 'multiverseId'], ascending=False)\n",
    "         .groupby('name')\n",
    "         .first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before groupby().last(): 56_002, 78\n",
    "# after: 21_814, 77\n",
    "cards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cards = set(cards.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can eventually use this dataframe to create a generator of true card pairs off of a single card anchor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split, stratifying on card color identity, card type, rarity.\n",
    "\n",
    "we will split on cards. this is actually tricky, right? it would be easy if we could just do a 95/5/5 and then there was enough pairing between 5s and other 5s to build an entire test / val set, but I actually suspect we might have a problem fielding that many extra records. oh well, I guess we'll tell in due time\n",
    "\n",
    "since we want to stratify on so many things, and we have a 2/3s chance of any card being in the true label, I actually think fully random sampling is approporiate. we can look at the breakdown of that by other features if we need to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_train, cards_test_val = train_test_split(cards.index.values, test_size=.1, random_state=1337)\n",
    "cards_test, cards_val = train_test_split(cards_test_val, test_size=.5, random_state=1337)\n",
    "\n",
    "print(f\"\"\"\n",
    "train: {cards_train.shape[0]}\n",
    "test: {cards_test.shape[0]}\n",
    "val: {cards_val.shape[0]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert cards into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmc_map = {0.0: 'zero',\n",
    "           0.5: 'one half',\n",
    "           1.0: 'one',\n",
    "           2.0: 'two',\n",
    "           3.0: 'three',\n",
    "           4.0: 'four',\n",
    "           5.0: 'five',\n",
    "           6.0: 'six',\n",
    "           7.0: 'seven',\n",
    "           8.0: 'eight',\n",
    "           9.0: 'nine',\n",
    "           10.0: 'ten',\n",
    "           11.0: 'eleven',\n",
    "           12.0: 'twelve',\n",
    "           13.0: 'thirteen',\n",
    "           14.0: 'fourteen',\n",
    "           15.0: 'fifteen',\n",
    "           16.0: 'sixteen',\n",
    "           1000000.0: 'one million', }\n",
    "\n",
    "\n",
    "color_map = {'W': 'white', 'U': 'blue', 'B': 'black', 'R': 'red', 'G': 'green'}\n",
    "\n",
    "\n",
    "def parse_mana_colors_from_cost(mc):\n",
    "    return ', '.join(color_map[c] for c in 'WUBRG' if c in (mc or ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert parse_mana_colors_from_cost('{2}{U}{U}{B}') == 'blue, black'\n",
    "assert parse_mana_colors_from_cost('{8}{W}{W}') == 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_card_text(card):\n",
    "    mana_color_str = parse_mana_colors_from_cost(card.manaCost)\n",
    "    cmc_str = f\"{cmc_map[card.convertedManaCost]} mana\"\n",
    "    \n",
    "    if mana_color_str != '':\n",
    "        mana_color_str = f' including {mana_color_str}'\n",
    "    \n",
    "    return (f\"for {cmc_str}{mana_color_str}, cast {card.type} {card.name}: {card.text}\"\n",
    "            .lower()\n",
    "            .replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = cards_train[4]\n",
    "cards.loc[name]\n",
    "get_card_text(cards.loc[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_text = pd.DataFrame({'text': cards.apply(get_card_text, axis=1)})\n",
    "card_text.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's just go with this, see how it works out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a `huggingface` `datasets`\n",
    "\n",
    "following along with the relatively simple example [here](https://github.com/huggingface/datasets/blob/master/datasets/squad/squad.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### custom dataset loader?\n",
    "\n",
    "meh let's try the `csv` loader first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Edhrec(datasets.GeneratorBasedBuilder):\n",
    "#     raise NotImplementedError(\"havent written builder configs\")\n",
    "#     BUILDER_CONFIGS = []\n",
    "    \n",
    "#     def _info(self):\n",
    "#         return datasets.DatasetInfo(\n",
    "#             description=\"lol no thanks\",\n",
    "#             features=datasets.Features({\"id\": datasets.Value('string'),\n",
    "#                                         \"text_a\": datasets.Value('string'),\n",
    "#                                         \"text_b\": datasets.Value('string'),\n",
    "#                                         \"label\": datasets.Value(\"int32\"), }),\n",
    "#             supervised_keys=None\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `csv` loader\n",
    "\n",
    "generate `csv`s the same way we were doing `parquet` (see appendix) and load those as datasets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def card_sampler(cardlist, num_pairs_per_card=10, exclude_cards=None):\n",
    "    exclude_cards = exclude_cards or set()\n",
    "    np.random.seed(1337)\n",
    "    neg_card_options = np.array(list(all_cards.difference(exclude_cards)))\n",
    "    for card in tqdm(cardlist):\n",
    "        rec = card_text.loc[card]\n",
    "        commanders = edhrec_cards[edhrec_cards.name == card].commander.unique()\n",
    "        \n",
    "        # get positive card labels\n",
    "        pos_options = (edhrec_cards[edhrec_cards.commander.isin(commanders)\n",
    "                                    & (edhrec_cards.name != card)\n",
    "                                    & (~edhrec_cards.name.isin(exclude_cards))]\n",
    "                       .name.unique())\n",
    "\n",
    "        n_labels = min(num_pairs_per_card, pos_options.shape[0])\n",
    "        pos_cards = np.random.choice(pos_options,\n",
    "                                     size=n_labels,\n",
    "                                     replace=False)\n",
    "        \n",
    "        for pos_card in pos_cards:\n",
    "            try:\n",
    "                pos_text_rec = card_text.loc[pos_card]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    pos_text_rec = card_text.loc[pos_card.replace('//', '/')]\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            yield (*rec, *pos_text_rec, True)\n",
    "        \n",
    "        # get negative samples:\n",
    "        # start by getting 3x as many as we had before, and assume that this\n",
    "        # will net at least x that aren't positive label options. additionally,\n",
    "        # we are comfortable having a full set of negative labels even if we\n",
    "        # had few (or no!) positive labels\n",
    "        neg_cards = np.random.choice(neg_card_options,\n",
    "                                     size=3 * num_pairs_per_card,\n",
    "                                     replace=False)\n",
    "        \n",
    "        neg_cards = list(set(neg_cards).difference(pos_cards))[:n_labels]\n",
    "        for neg_card in neg_cards:\n",
    "            neg_text_rec = card_text.loc[neg_card]\n",
    "            yield (*rec, *neg_text_rec, False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def grouper(iterable, n, fillvalue=None):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return itertools.zip_longest(*args, fillvalue=fillvalue)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!rm -r ./data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for split_type in ['train', 'test', 'val']:\n",
    "    os.makedirs(os.path.join('.', 'data', split_type), exist_ok=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!tree ."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NUM_PAIRS_PER_CARD = 100\n",
    "PARQUET_CHUNK_SIZE = 100_000\n",
    "\n",
    "for (split_type, cardlist, exclude_cardlist) in [['val', cards_val, None],\n",
    "                                                 ['test', cards_test, None],\n",
    "                                                 ['train', cards_train, set(cards_val).union(cards_test)], ]:\n",
    "    root = os.path.join('.', split_type)\n",
    "    sampler = card_sampler(cardlist, NUM_PAIRS_PER_CARD, exclude_cardlist)\n",
    "    for (i, grp) in enumerate(tqdm(grouper(sampler, PARQUET_CHUNK_SIZE))):\n",
    "        (pd.DataFrame(grp, columns=['text_a', 'text_b', 'label'])\n",
    "         .dropna()\n",
    "         .to_csv(os.path.join('.', 'data', split_type, f'part-{i:0>5}.csv'),\n",
    "                 index=False,\n",
    "                 quoting=csv.QUOTE_ALL))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!tree ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading csvs, shuffling, tokenizing, etc datasets now\n",
    "\n",
    "+ tokenizing from [here](https://huggingface.co/docs/datasets/processing.html#processing-data-in-batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_DS_CACHE = os.path.join('.', 'data', 'edhrec_cache_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_LENGTH = 300\n",
    "\n",
    "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# def tokenizer_map_func(rec):\n",
    "#     return tokenizer(rec['text_a'], rec['text_b'],\n",
    "#                      return_tensors='np',\n",
    "#                      padding='max_length',\n",
    "#                      max_length=MAX_LENGTH,\n",
    "#                      truncation=True)\n",
    "\n",
    "\n",
    "# (load_dataset('csv',\n",
    "#               data_files={split_type: glob(os.path.join('.', 'data', split_type, '*.csv'))\n",
    "#                           for split_type in ['val', 'test', 'train']},\n",
    "#               quoting=csv.QUOTE_ALL)\n",
    "#  .shuffle()\n",
    "#  .map(tokenizer_map_func,\n",
    "#       batched=True)\n",
    "#  .save_to_disk(F_DS_CACHE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(F_DS_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['val'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total # of training epochs\n",
    "    per_device_train_batch_size=64,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=1,\n",
    "    # my custom ones\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy='steps',\n",
    "    logging_first_step=True,\n",
    "    no_cuda=True,\n",
    "    seed=1337,\n",
    "    dataloader_drop_last=True,\n",
    "    dataloader_num_workers=30,\n",
    "    label_names=['label'],\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=dataset['test'],      # training dataset\n",
    "    eval_dataset=dataset['val'],         # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make deck predictions for one of my existing decks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# appendix\n",
    "\n",
    "the following is either hacking, didn't work, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizing sentences\n",
    "\n",
    "~~we will be reusing most of the text sentences above several times; might as well tokenize them all up front once instead of tokenizing most of them 100x later~~\n",
    "\n",
    "just do shit the way the documenation suggests we should. do them on the completely built pair parquet files below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaTokenizerFast\n",
    "# tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_tokenizer(row, *args, **kwargs):\n",
    "#     return pd.Series(tokenizer(row.text, *args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (card_text.head(20)\n",
    "#  .apply(my_tokenizer, axis=1, truncation=True, padding=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# card_text = (card_text\n",
    "#              .join(card_text\n",
    "#                    .apply(my_tokenizer, axis=1, truncation=True, padding=True)))\n",
    "\n",
    "# card_text.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### making the pair suggestions dataset\n",
    "\n",
    "okay so we have\n",
    "\n",
    "1. a train / test / val split of all cards\n",
    "1. a series of card text values (our \"sentences\")\n",
    "1. a list of `card --> deck` relationships\n",
    "\n",
    "the task now is to\n",
    "\n",
    "1. generate positive and negative cases for each card\n",
    "    + positive: `card --> deck <-- card`\n",
    "    + negative: just not that\n",
    "1. look up their text values\n",
    "1. write those values to file\n",
    "    + probably want to chunk this up somehow, maybe write 1k sentences per parquet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def card_sampler(cardlist, num_pairs_per_card=10, exclude_cards=None):\n",
    "    exclude_cards = exclude_cards or set()\n",
    "    np.random.seed(1337)\n",
    "    neg_card_options = np.array(list(all_cards.difference(exclude_cards)))\n",
    "    for card in tqdm(cardlist):\n",
    "        rec = card_text.loc[card]\n",
    "        commanders = edhrec_cards[edhrec_cards.name == card].commander.unique()\n",
    "        \n",
    "        # get positive card labels\n",
    "        pos_options = (edhrec_cards[edhrec_cards.commander.isin(commanders)\n",
    "                                    & (edhrec_cards.name != card)\n",
    "                                    & (~edhrec_cards.name.isin(exclude_cards))]\n",
    "                       .name.unique())\n",
    "\n",
    "        n_labels = min(num_pairs_per_card, pos_options.shape[0])\n",
    "        pos_cards = np.random.choice(pos_options,\n",
    "                                     size=n_labels,\n",
    "                                     replace=False)\n",
    "        \n",
    "        for pos_card in pos_cards:\n",
    "            try:\n",
    "                pos_text_rec = card_text.loc[pos_card]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    pos_text_rec = card_text.loc[pos_card.replace('//', '/')]\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            yield (*rec, *pos_text_rec, True)\n",
    "        \n",
    "        # get negative samples\n",
    "        # start by getting 3x as many as we had before, and assume that this\n",
    "        # will net at least x that weren't options before.\n",
    "        # additionally, we are comfortable having a full set of negative labels\n",
    "        # even if we had few (or no!) positive labels\n",
    "        neg_cards = np.random.choice(neg_card_options,\n",
    "                                     size=3 * num_pairs_per_card,\n",
    "                                     replace=False)\n",
    "        \n",
    "        neg_cards = list(set(neg_cards).difference(pos_cards))[:n_labels]\n",
    "        for neg_card in neg_cards:\n",
    "            neg_text_rec = card_text.loc[neg_card]\n",
    "            yield (*rec, *neg_text_rec, False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!rm -r ./data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# import os\n",
    "\n",
    "# for split_type in ['train', 'test', 'val']:\n",
    "#     for label_type in ['pos', 'neg']:\n",
    "#         os.makedirs(os.path.join('.', 'data', split_type, label_type), exist_ok=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "for split_type in ['train', 'test', 'val']:\n",
    "    os.makedirs(os.path.join('.', 'data', split_type), exist_ok=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!tree ."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import itertools\n",
    "\n",
    "def grouper(iterable, n, fillvalue=None):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return itertools.zip_longest(*args, fillvalue=fillvalue)\n",
    "\n",
    "# for g in grouper(range(20), 3):\n",
    "#     print(type(g))\n",
    "#     print(g)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# for g in grouper(card_sampler(cards_val[:3], 10), 4):\n",
    "#     for rec in g:\n",
    "#         print(rec)\n",
    "#     print()\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "NUM_PAIRS_PER_CARD = 100\n",
    "PARQUET_CHUNK_SIZE = 100_000\n",
    "\n",
    "for (split_type, cardlist, exclude_cardlist) in [['val', cards_val, None],\n",
    "                                                 ['test', cards_test, None],\n",
    "                                                 ['train', cards_train, set(cards_val).union(cards_test)], ]:\n",
    "    root = os.path.join('.', split_type)\n",
    "    sampler = card_sampler(cardlist, NUM_PAIRS_PER_CARD, exclude_cardlist)\n",
    "    for (i, grp) in enumerate(tqdm(grouper(sampler, PARQUET_CHUNK_SIZE))):\n",
    "        (pd.DataFrame(grp, columns=['text_a', 'text_b', 'label'])\n",
    "         .dropna()\n",
    "         .to_parquet(os.path.join('.', 'data', split_type, f'part-{i:0>5}.parquet')))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!tree ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the pytorch datasets\n",
    "\n",
    "basing this in large part off of [this doc page](https://huggingface.co/transformers/custom_datasets.html#nlplib)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_parquet('./data/train')\n",
    "test = pd.read_parquet('./data/test')\n",
    "val = pd.read_parquet('./data/val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### do the encodings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_encodings = tokenizer(test.text_a.values.tolist(),\n",
    "                           test.text_b.values.tolist(),\n",
    "                           truncation=True,\n",
    "                           padding=True)\n",
    "\n",
    "with open('test_encodings.pkl', 'wb') as fp:\n",
    "    pickle.dump(test_encodings, fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('test_encodings.pkl', 'rb') as fp:\n",
    "    test_encodings = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val_encodings = tokenizer(val.text_a.values.tolist(),\n",
    "                          val.text_b.values.tolist(),\n",
    "                          truncation=True,\n",
    "                          padding=True)\n",
    "\n",
    "with open('val_encodings.pkl', 'wb') as fp:\n",
    "    pickle.dump(test_encodings, fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('val_encodings.pkl', 'rb') as fp:\n",
    "    val_encodings = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[len(_) for _ in val_encodings['input_ids'][:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, the below killed the kernel... :("
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "train_encodings = tokenizer(train.text_a.values.tolist(),\n",
    "                            train.text_b.values.tolist(),\n",
    "                            truncation=True,\n",
    "                            padding=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "with open('train_encodings.pkl', 'wb') as fp:\n",
    "    pickle.dump(train_encodings, fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "with open('train_encodings.pkl', 'rb') as fp:\n",
    "    train_encodings = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "idx = 0\n",
    "rec = train.iloc[idx]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "class EdhrecDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
