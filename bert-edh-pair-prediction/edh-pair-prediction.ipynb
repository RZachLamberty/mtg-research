{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDH card pair prediction\n",
    "\n",
    "1. build train / test / dev data set\n",
    "    1. get EDH card pair recommendations from edhrec.com. these have prediction value 1\n",
    "    1. generate false pairs (prediction value 0) by randomly generating pairs\n",
    "    1. split, stratifying on card color identity, card type, rarity.\n",
    "    1. convert cards into sentences\n",
    "1. fine-tune\n",
    "    1. load pre-trained bert model on prediction task \"card a, card b --> {yes,no} was edh rec\n",
    "1. make deck predictions for one of my existing decks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (BertConfig, BertTokenizerFast,\n",
    "                          BertForNextSentencePrediction,\n",
    "                          DataCollatorWithPadding,\n",
    "                          PreTrainedModel, PreTrainedTokenizerFast,\n",
    "                          Trainer, TrainingArguments, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build train / test / dev data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get EDH card pair recommendations from edhrec.com\n",
    "\n",
    "these will have prediction value 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mtg.cards\n",
    "import mtg.extract.edhrec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edhrec_cards = (mtg.extract.edhrec.get_commanders_and_cards()\n",
    "                [['name', 'commander']])\n",
    "edhrec_cards.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common cards\n",
    "edhrec_cards.name.value_counts().plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common cards\n",
    "vc = edhrec_cards.name.value_counts()\n",
    "vc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most cards appear in only 1 commander recs, up to 500 cards appear\n",
    "# in 10 commander recs\n",
    "vc[vc <= 10].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we just ran with this, how many total pairs could we generate this way? basically, for every card in deck X, every other card is a valid pair. that's:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "z = edhrec_cards.commander.value_counts()\n",
    "f\"{int((z * (z - 1) / 2).sum()):,}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at first I was going to say no way, buuuuuut it's actually not terrible... we want big data, after all\n",
    "\n",
    "we would need to generate about 32 min negative labels if that were the dataset we were interested in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get all cards from mtgjson\n",
    "\n",
    "to generate false pairs we will randomly select from all cards. about 65% of all MTG cards are referenced on edhrec, but the rest are also, presumably, good choices for 0 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards = (mtg.cards.cards_df()\n",
    "         .sort_values(by=['name', 'multiverseId'], ascending=False)\n",
    "         .groupby('name')\n",
    "         .first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before groupby().last(): 56_002, 78\n",
    "# after: 21_814, 77\n",
    "cards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cards = set(cards.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can eventually use this dataframe to create a generator of true card pairs off of a single card anchor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split, stratifying on card color identity, card type, rarity.\n",
    "\n",
    "we will split on cards. this is actually tricky, right? it would be easy if we could just do a 95/5/5 and then there was enough pairing between 5s and other 5s to build an entire test / val set, but I actually suspect we might have a problem fielding that many extra records. oh well, I guess we'll tell in due time\n",
    "\n",
    "since we want to stratify on so many things, and we have a 2/3s chance of any card being in the true label, I actually think fully random sampling is approporiate. we can look at the breakdown of that by other features if we need to"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cards_train, cards_test_val = train_test_split(cards.index.values, test_size=.1, random_state=1337)\n",
    "cards_test, cards_val = train_test_split(cards_test_val, test_size=.5, random_state=1337)\n",
    "\n",
    "print(f\"\"\"\n",
    "train: {cards_train.shape[0]}\n",
    "test: {cards_test.shape[0]}\n",
    "val: {cards_val.shape[0]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert cards into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmc_map = {0.0: 'zero',\n",
    "           0.5: 'one half',\n",
    "           1.0: 'one',\n",
    "           2.0: 'two',\n",
    "           3.0: 'three',\n",
    "           4.0: 'four',\n",
    "           5.0: 'five',\n",
    "           6.0: 'six',\n",
    "           7.0: 'seven',\n",
    "           8.0: 'eight',\n",
    "           9.0: 'nine',\n",
    "           10.0: 'ten',\n",
    "           11.0: 'eleven',\n",
    "           12.0: 'twelve',\n",
    "           13.0: 'thirteen',\n",
    "           14.0: 'fourteen',\n",
    "           15.0: 'fifteen',\n",
    "           16.0: 'sixteen',\n",
    "           1000000.0: 'one million', }\n",
    "\n",
    "\n",
    "color_map = {'W': 'white', 'U': 'blue', 'B': 'black', 'R': 'red', 'G': 'green'}\n",
    "\n",
    "\n",
    "def parse_mana_colors_from_cost(mc):\n",
    "    return ', '.join(color_map[c] for c in 'WUBRG' if c in (mc or ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert parse_mana_colors_from_cost('{2}{U}{U}{B}') == 'blue, black'\n",
    "assert parse_mana_colors_from_cost('{8}{W}{W}') == 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_card_text(card):\n",
    "    mana_color_str = parse_mana_colors_from_cost(card.manaCost)\n",
    "    cmc_str = f\"{cmc_map[card.convertedManaCost]} mana\"\n",
    "    \n",
    "    if mana_color_str != '':\n",
    "        mana_color_str = f' including {mana_color_str}'\n",
    "    \n",
    "    return (f\"for {cmc_str}{mana_color_str}, cast {card.type} {card.name}: {card.text}\"\n",
    "            .lower()\n",
    "            .replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "name = cards_train[4]\n",
    "cards.loc[name]\n",
    "get_card_text(cards.loc[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_text = pd.DataFrame({'text': cards.apply(get_card_text, axis=1)})\n",
    "card_text.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's just go with this, see how it works out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a `huggingface` `datasets`\n",
    "\n",
    "following along with the relatively simple example [here](https://github.com/huggingface/datasets/blob/master/datasets/squad/squad.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### custom dataset loader?\n",
    "\n",
    "meh let's try the `csv` loader first"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class Edhrec(datasets.GeneratorBasedBuilder):\n",
    "    raise NotImplementedError(\"havent written builder configs\")\n",
    "    BUILDER_CONFIGS = []\n",
    "    \n",
    "    def _info(self):\n",
    "        return datasets.DatasetInfo(\n",
    "            description=\"lol no thanks\",\n",
    "            features=datasets.Features({\"id\": datasets.Value('string'),\n",
    "                                        \"text_a\": datasets.Value('string'),\n",
    "                                        \"text_b\": datasets.Value('string'),\n",
    "                                        \"label\": datasets.Value(\"int32\"), }),\n",
    "            supervised_keys=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `csv` loader\n",
    "\n",
    "generate `csv`s the same way we were doing `parquet` (see appendix) and load those as datasets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def card_sampler(cardlist, num_pairs_per_card=10, exclude_cards=None):\n",
    "    exclude_cards = exclude_cards or set()\n",
    "    np.random.seed(1337)\n",
    "    neg_card_options = np.array(list(all_cards.difference(exclude_cards)))\n",
    "    for card in tqdm(cardlist):\n",
    "        rec = card_text.loc[card]\n",
    "        commanders = edhrec_cards[edhrec_cards.name == card].commander.unique()\n",
    "        \n",
    "        # get positive card labels\n",
    "        pos_options = (edhrec_cards[edhrec_cards.commander.isin(commanders)\n",
    "                                    & (edhrec_cards.name != card)\n",
    "                                    & (~edhrec_cards.name.isin(exclude_cards))]\n",
    "                       .name.unique())\n",
    "\n",
    "        n_labels = min(num_pairs_per_card, pos_options.shape[0])\n",
    "        pos_cards = np.random.choice(pos_options,\n",
    "                                     size=n_labels,\n",
    "                                     replace=False)\n",
    "        \n",
    "        for pos_card in pos_cards:\n",
    "            try:\n",
    "                pos_text_rec = card_text.loc[pos_card]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    pos_text_rec = card_text.loc[pos_card.replace('//', '/')]\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            yield (*rec, *pos_text_rec, True)\n",
    "        \n",
    "        # get negative samples:\n",
    "        # start by getting 3x as many as we had before, and assume that this\n",
    "        # will net at least x that aren't positive label options. additionally,\n",
    "        # we are comfortable having a full set of negative labels even if we\n",
    "        # had few (or no!) positive labels\n",
    "        neg_cards = np.random.choice(neg_card_options,\n",
    "                                     size=3 * num_pairs_per_card,\n",
    "                                     replace=False)\n",
    "        \n",
    "        neg_cards = list(set(neg_cards).difference(pos_cards))[:n_labels]\n",
    "        for neg_card in neg_cards:\n",
    "            neg_text_rec = card_text.loc[neg_card]\n",
    "            yield (*rec, *neg_text_rec, False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def grouper(iterable, n, fillvalue=None):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return itertools.zip_longest(*args, fillvalue=fillvalue)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!rm -r ./data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for split_type in ['train', 'test', 'val']:\n",
    "    os.makedirs(os.path.join('.', 'data', split_type), exist_ok=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!tree ."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NUM_PAIRS_PER_CARD = 100\n",
    "PARQUET_CHUNK_SIZE = 100_000\n",
    "\n",
    "for (split_type, cardlist, exclude_cardlist) in [['val', cards_val, None],\n",
    "                                                 ['test', cards_test, None],\n",
    "                                                 ['train', cards_train, set(cards_val).union(cards_test)], ]:\n",
    "    root = os.path.join('.', split_type)\n",
    "    sampler = card_sampler(cardlist, NUM_PAIRS_PER_CARD, exclude_cardlist)\n",
    "    for (i, grp) in enumerate(tqdm(grouper(sampler, PARQUET_CHUNK_SIZE))):\n",
    "        (pd.DataFrame(grp, columns=['text_a', 'text_b', 'label'])\n",
    "         .dropna()\n",
    "         .to_csv(os.path.join('.', 'data', split_type, f'part-{i:0>5}.csv'),\n",
    "                 index=False,\n",
    "                 quoting=csv.QUOTE_ALL))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!tree ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading csvs, shuffling, tokenizing, etc datasets now\n",
    "\n",
    "+ tokenizing from [here](https://huggingface.co/docs/datasets/processing.html#processing-data-in-batches)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# F_DS_CACHE = os.path.join('.', 'data', 'edhrec_cache_dataset')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MAX_LENGTH = 300\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenizer_map_func(rec):\n",
    "    return tokenizer(rec['text_a'], rec['text_b'],\n",
    "                     padding='max_length',\n",
    "                     max_length=MAX_LENGTH,\n",
    "                     truncation=True)\n",
    "\n",
    "\n",
    "def fix_label(rec):\n",
    "    return {'label_as_int': [int(_) for _ in rec['label']]}\n",
    "\n",
    "\n",
    "split_types = ['val', 'test', 'train']\n",
    "dataset = (load_dataset('csv',\n",
    "                        data_files={split_type: glob(os.path.join('.', 'data', split_type, '*.csv'))\n",
    "                                    for split_type in split_types},\n",
    "                        quoting=csv.QUOTE_ALL)\n",
    "           #.map(fix_label,\n",
    "           #     batched=True)\n",
    "           .shuffle(seeds={split_type: 1337\n",
    "                           for split_type in split_types})\n",
    "           .map(tokenizer_map_func,\n",
    "                batched=True))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# dataset = load_from_disk(F_DS_CACHE)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# dataset['val'][0]['label_as_int']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine-tune"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "\n",
    "TRAIN_BATCH = 36\n",
    "EVAL_BATCH = 36\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                   # output directory\n",
    "    num_train_epochs=1,                       # total # of training epochs\n",
    "    per_device_train_batch_size=TRAIN_BATCH,  # batch size per device during training\n",
    "    per_device_eval_batch_size=EVAL_BATCH,    # batch size for evaluation\n",
    "    warmup_steps=500,                         # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                        # strength of weight decay\n",
    "    logging_dir='./logs',                     # directory for storing logs\n",
    "    # my custom ones\n",
    "    logging_steps=250,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy='steps',\n",
    "    logging_first_step=True,\n",
    "    seed=1337,\n",
    "    dataloader_drop_last=True,\n",
    "    dataloader_num_workers=30,\n",
    "    label_names=['labels'],\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=10,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                              # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                       # training arguments, defined above\n",
    "    train_dataset=dataset['train'],           # training dataset\n",
    "    eval_dataset=dataset['val'].select(range(EVAL_BATCH * 1_000)),  # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_path = \"edhrec-bert-base-uncased\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainer.evaluate(dataset['val'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainer.evaluate(dataset['test'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainer.predict?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "p = trainer.predict(dataset['test'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset['test']['text_a'][0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset['test']['text_b'][0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "zero_max = p.predictions[:, 0].argmax()\n",
    "p.predictions[zero_max]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset['test']['text_a'][zero_max]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset['test']['text_b'][zero_max]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "one_max = p.predictions[:, 1].argmax()\n",
    "p.predictions[one_max]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset['test']['text_a'][one_max]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset['test']['text_b'][one_max]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{k: np.array(v) for (k, v) in dataset['test'][:10].items() if k in ['attention_mask', 'input_ids', 'label', 'token_type_ids']}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "(model(**{k: torch.as_tensor(np.array(v)).to(\"cuda\")\n",
    "          for (k, v) in dataset['test'][:10].items()\n",
    "          if k in ['attention_mask', 'input_ids', 'token_type_ids']})\n",
    " [0]\n",
    " .softmax(1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset['test']['label'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## double-checking our trained model\n",
    "\n",
    "next steps\n",
    "\n",
    "+ what do our false positives look like\n",
    "+ what is the separation like for \"cards that have been on edhrec\" vs. \"cards that havent\n",
    "    + i.e. do we just predict \"both cards have been on EDHREC\"?\n",
    "    + did we create a dataset that is just (edhrec cards, either type)? I thought we were making (either type, either type)\n",
    "+ what is the sorted list of recommendations given an existing deck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why are these all only edhrec cards? I thought I was generating pairs from both sides?\n",
    "\n",
    "is this a problem? when a new card shows up and has never been seen before, will the model be unable to handle it? I think not, because presumably there were cards in test / val that it had never seen before (have I verified that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the trained model\n",
    "config = BertConfig.from_pretrained('edhrec-bert-base-uncased')\n",
    "model = BertForNextSentencePrediction.from_pretrained('edhrec-bert-base-uncased', config=config)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('edhrec-bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_BATCH = 36\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./ignore',\n",
    "    per_device_eval_batch_size=EVAL_BATCH,    # batch size for evaluation\n",
    "    label_names=['labels'],\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 300\n",
    "\n",
    "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenizer_map_func(rec):\n",
    "    return tokenizer(rec['text_a'], rec['text_b'],\n",
    "                     padding='max_length',\n",
    "                     max_length=MAX_LENGTH,\n",
    "                     truncation=True)\n",
    "\n",
    "\n",
    "def fix_label(rec):\n",
    "    return {'label_as_int': [int(_) for _ in rec['label']]}\n",
    "\n",
    "\n",
    "split_types = ['val', 'test', 'train']\n",
    "dataset = (load_dataset('csv',\n",
    "                        data_files={split_type: sorted(glob(os.path.join('.', 'data', split_type, '*.csv')))\n",
    "                                    for split_type in split_types},\n",
    "                        quoting=csv.QUOTE_ALL)\n",
    "           #.map(fix_label,\n",
    "           #     batched=True)\n",
    "           .shuffle(seeds={split_type: 1337\n",
    "                           for split_type in split_types})\n",
    "           .map(tokenizer_map_func,\n",
    "                batched=True))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p = trainer.predict(dataset['test'].select(range(1_000)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p.predictions.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p.label_ids.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p.metrics"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "probs = softmax(p.predictions, axis=1)\n",
    "\n",
    "z = pd.DataFrame({'p1': probs[:, 1],\n",
    "                  'y_pred': probs.argmax(axis=1),\n",
    "                  'y': p.label_ids})\n",
    "z.loc[:, 'is_right'] = z.y == z.y_pred\n",
    "\n",
    "z.sort_values(by='p1', ascending=False, inplace=True)\n",
    "z.reset_index(drop=True, inplace=True)\n",
    "\n",
    "z.is_right.value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "total_true = z.y.sum()\n",
    "\n",
    "(z.y.cumsum() / total_true).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "+ make the combo dataframe\n",
    "+ convert that into a dataset (probably a `.from_pandas` or some shit)\n",
    "+ pass that to eval\n",
    "+ sort by predictions\n",
    "+ profit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!ls -alh ../../mtg/mtg/tags/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!touch ~/.virtualenvs/mtg-research/src/mtg/mtg/tags.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the following on a computer with `mtg` installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kykar_cards = [\n",
    "    \"Aetherflux Reservoir\",\n",
    "    \"Anointed Procession\",\n",
    "    \"As Foretold\",\n",
    "    \"Austere Command\",\n",
    "    \"Azorius Chancery\",\n",
    "    \"Azorius Signet\",\n",
    "    \"Baral, Chief of Compliance\",\n",
    "    \"Blue Sun's Zenith\",\n",
    "    \"Boros Charm\",\n",
    "    \"Boros Garrison\",\n",
    "    \"Boros Signet\",\n",
    "    \"Cascade Bluffs\",\n",
    "    \"Chaos Warp\",\n",
    "    \"Chromatic Lantern\",\n",
    "    \"Command Tower\",\n",
    "    \"Commander's Sphere\",\n",
    "    \"Counterspell\",\n",
    "    \"Cultivator's Caravan\",\n",
    "    \"Cyclonic Rift\",\n",
    "    \"Desolate Lighthouse\",\n",
    "    \"Disallow\",\n",
    "    \"Dismantling Blow\",\n",
    "    \"Docent of Perfection\",\n",
    "    \"Dovin's Veto\",\n",
    "    \"Eerie Interlude\",\n",
    "    \"Esper Panorama\",\n",
    "    \"Fact or Fiction\",\n",
    "    \"Faithless Looting\",\n",
    "    \"Flood Plain\",\n",
    "    \"Gitaxian Probe\",\n",
    "    \"Glacial Fortress\",\n",
    "    \"Grixis Panorama\",\n",
    "    \"Guttersnipe\",\n",
    "    \"Hallowed Fountain\",\n",
    "    \"Impulse\",\n",
    "    \"Izzet Boilerworks\",\n",
    "    \"Izzet Signet\",\n",
    "    \"Kor Haven\",\n",
    "    \"Kykar, Wind's Fury\",\n",
    "    \"Mentor of the Meek\",\n",
    "    \"Mind Stone\",\n",
    "    \"Mizzix of the Izmagnus\",\n",
    "    \"Mizzix's Mastery\",\n",
    "    \"Murmuring Mystic\",\n",
    "    \"Mystic Confluence\",\n",
    "    \"Mystic Monastery\",\n",
    "    \"Mystic Speculation\",\n",
    "    \"Mystical Tutor\",\n",
    "    \"Narset Transcendent\",\n",
    "    \"Needle Spires\",\n",
    "    \"Neurok Stealthsuit\",\n",
    "    \"Nimbus Maze\",\n",
    "    \"Niv-Mizzet, Parun\",\n",
    "    \"Omniscience\",\n",
    "    \"Ponder\",\n",
    "    \"Port Town\",\n",
    "    \"Prairie Stream\",\n",
    "    \"Preordain\",\n",
    "    \"Primal Amulet\",\n",
    "    \"Ral, Izzet Viceroy\",\n",
    "    \"Reliquary Tower\",\n",
    "    \"Render Silent\",\n",
    "    \"Rhystic Study\",\n",
    "    \"Sacred Foundry\",\n",
    "    \"Sea of Clouds\",\n",
    "    \"Seachrome Coast\",\n",
    "    \"Serum Visions\",\n",
    "    \"Sol Ring\",\n",
    "    \"Spirebluff Canal\",\n",
    "    \"Sram's Expertise\",\n",
    "    \"Steam Vents\",\n",
    "    \"Stroke of Genius\",\n",
    "    \"Sulfur Falls\",\n",
    "    \"Sunforger\",\n",
    "    \"Supreme Verdict\",\n",
    "    \"Swords to Plowshares\",\n",
    "    \"Taigam, Ojutai Master\",\n",
    "    \"Talrand, Sky Summoner\",\n",
    "    \"Teferi, Hero of Dominaria\",\n",
    "    \"Teferi, Time Raveler\",\n",
    "    \"Temple of Enlightenment\",\n",
    "    \"Temple of Epiphany\",\n",
    "    \"The Locust God\",\n",
    "    \"Thought Vessel\",\n",
    "    \"Tidespout Tyrant\",\n",
    "    \"Trail of Evidence\",\n",
    "    \"Vandalblast\",\n",
    "    \"Young Pyromancer\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infinite_purphoros = [\n",
    "    \"Combustible Gearhulk\",\n",
    "    \"Hellkite Charger\",\n",
    "    \"Inferno Titan\",\n",
    "    \"Neheb, the Eternal\",\n",
    "    \"Tyrant's Familiar\",\n",
    "    \"Urabrask the Hidden\",\n",
    "    \"Zealous Conscripts\",\n",
    "    \"Braid of Fire\",\n",
    "    \"Impact Tremors\",\n",
    "    \"Seething Song\",\n",
    "    \"Sundial of the Infinite\",\n",
    "    \"Purphoros, Bronze-Blooded\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\n",
    "    \"Advent of the Wurm\", \n",
    "    \"Ajani, Mentor of Heroes\", \n",
    "    \"Akroma's Memorial\", \n",
    "    \"Archangel of Thune\", \n",
    "    \"Armada Wurm\", \n",
    "    \"Austere Command\", \n",
    "    \"Avenger of Zendikar\", \n",
    "    \"Blighted Woodland\", \n",
    "    \"Blossoming Sands\", \n",
    "    \"Bow of Nylea\", \n",
    "    \"Brushland\", \n",
    "    \"Caged Sun\", \n",
    "    \"Champion of Lambholt\", \n",
    "    \"Command Tower\", \n",
    "    \"Constant Mists\", \n",
    "    \"Courser of Kruphix\", \n",
    "    \"Cultivate\", \n",
    "    \"Darien, King of Kjeldor\", \n",
    "    \"Doubling Season\", \n",
    "    \"Elfhame Palace\", \n",
    "    \"Elspeth, Sun's Champion\", \n",
    "    \"Emmara Tandris\", \n",
    "    \"Evolving Wilds\", \n",
    "    \"Forest\", \n",
    "    \"Geist-Honored Monk\", \n",
    "    \"Giant Adephage\", \n",
    "    \"Graypelt Refuge\", \n",
    "    \"Green Sun's Zenith\", \n",
    "    \"Grove of the Guardian\", \n",
    "    \"Growing Ranks\", \n",
    "    \"Hornet Queen\", \n",
    "    \"Hydra Broodmaster\", \n",
    "    \"Incremental Growth\", \n",
    "    \"Into the Wilds\", \n",
    "    \"Kodama's Reach\", \n",
    "    \"Krosan Verge\", \n",
    "    \"Meadowboon\", \n",
    "    \"Mikaeus, the Lunarch\", \n",
    "    \"Mimic Vat\", \n",
    "    \"Mirari's Wake\", \n",
    "    \"Nature's Lore\", \n",
    "    \"Nissa's Renewal\", \n",
    "    \"Nissa, Voice of Zendikar\", \n",
    "    \"Nylea, God of the Hunt\", \n",
    "    \"Oblivion Ring\", \n",
    "    \"Oracle of Mul Daya\", \n",
    "    \"Parallel Lives\", \n",
    "    \"Phyrexian Processor\", \n",
    "    \"Phyrexian Rebirth\", \n",
    "    \"Plains\", \n",
    "    \"Primal Vigor\", \n",
    "    \"Rampaging Baloths\", \n",
    "    \"Rancor\", \n",
    "    \"Razorverge Thicket\", \n",
    "    \"Reap What Is Sown\", \n",
    "    \"Reliquary Tower\", \n",
    "    \"Restoration Angel\", \n",
    "    \"Rhys the Redeemed\", \n",
    "    \"Riftstone Portal\", \n",
    "    \"Rupture Spire\", \n",
    "    \"Sakura-Tribe Elder\", \n",
    "    \"Second Harvest\", \n",
    "    \"Selesnya Charm\", \n",
    "    \"Selesnya Sanctuary\", \n",
    "    \"Selesnya Signet\", \n",
    "    \"Skyshroud Claim\", \n",
    "    \"Slime Molding\", \n",
    "    \"Spawnwrithe\", \n",
    "    \"Sundering Growth\", \n",
    "    \"Sunpetal Grove\", \n",
    "    \"Temple Garden\", \n",
    "    \"Terminus\", \n",
    "    \"Tireless Tracker\", \n",
    "    \"Transguild Promenade\", \n",
    "    \"Trostani's Summoner\", \n",
    "    \"Trostani, Selesnya's Voice\", \n",
    "    \"Vitu-Ghazi, the City-Tree\", \n",
    "    \"Voice of Resurgence\", \n",
    "    \"Wayfaring Temple\", \n",
    "    \"Windswept Heath\", \n",
    "    \"Worldspine Wurm\", \n",
    "    \"Worn Powerstone\", \n",
    "    \"Wrath of God\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goblins = [\n",
    "    \"Ash Barrens\",\n",
    "    \"Auntie's Hovel\",\n",
    "    \"Battle Squadron\",\n",
    "    \"Beetleback Chief\",\n",
    "    \"Blasphemous Act\",\n",
    "    \"Blood Crypt\",\n",
    "    \"Bloodfell Caves\",\n",
    "    \"Bloodmark Mentor\",\n",
    "    \"Boggart Harbinger\",\n",
    "    \"Boggart Mob\",\n",
    "    \"Boggart Shenanigans\",\n",
    "    \"Brightstone Ritual\",\n",
    "    \"Chandra Ablaze\",\n",
    "    \"Cinder Barrens\",\n",
    "    \"Coat of Arms\",\n",
    "    \"Command Tower\",\n",
    "    \"Commander's Sphere\",\n",
    "    \"Diabolic Tutor\",\n",
    "    \"Door of Destinies\",\n",
    "    \"Dreadbore\",\n",
    "    \"Earwig Squad\",\n",
    "    \"Empty the Warrens\",\n",
    "    \"Fatal Push\",\n",
    "    \"Fervor\",\n",
    "    \"Foreboding Ruins\",\n",
    "    \"Frenzied Goblin\",\n",
    "    \"Frogtosser Banneret\",\n",
    "    \"Gempalm Incinerator\",\n",
    "    \"Ghost Quarter\",\n",
    "    \"Goblin Charbelcher\",\n",
    "    \"Goblin Chieftain\",\n",
    "    \"Goblin Grenade\",\n",
    "    \"Goblin King\",\n",
    "    \"Goblin Lackey\",\n",
    "    \"Goblin Matron\",\n",
    "    \"Goblin Offensive\",\n",
    "    \"Goblin Piledriver\",\n",
    "    \"Goblin Rabblemaster\",\n",
    "    \"Goblin Razerunners\",\n",
    "    \"Goblin Recruiter\",\n",
    "    \"Goblin Ringleader\",\n",
    "    \"Goblin Sharpshooter\",\n",
    "    \"Goblin War Strike\",\n",
    "    \"Goblin Warchief\",\n",
    "    \"Grenzo, Dungeon Warden\",\n",
    "    \"Grenzo, Havoc Raiser\",\n",
    "    \"Hammer of Purphoros\",\n",
    "    \"Havoc Festival\",\n",
    "    \"Hordeling Outburst\",\n",
    "    \"Impact Tremors\",\n",
    "    \"Kiki-Jiki, Mirror Breaker\",\n",
    "    \"Knucklebone Witch\",\n",
    "    \"Krenko, Mob Boss\",\n",
    "    \"Lightning Crafter\",\n",
    "    \"Mad Auntie\",\n",
    "    \"Mana Echoes\",\n",
    "    \"Mogg Infestation\",\n",
    "    \"Mogg War Marshal\",\n",
    "    \"Mountain\",\n",
    "    \"Nykthos, Shrine to Nyx\",\n",
    "    \"Phyrexian Arena\",\n",
    "    \"Purphoros, God of the Forge\",\n",
    "    \"Quest for the Goblin Lord\",\n",
    "    \"Rakdos Carnarium\",\n",
    "    \"Rakdos's Return\",\n",
    "    \"Reckless One\",\n",
    "    \"Reliquary Tower\",\n",
    "    \"Ruby Medallion\",\n",
    "    \"Siege-Gang Commander\",\n",
    "    \"Smoldering Marsh\",\n",
    "    \"Sol Ring\",\n",
    "    \"Solemn Simulacrum\",\n",
    "    \"Stingscourger\",\n",
    "    \"Sulfuric Vortex\",\n",
    "    \"Swamp\",\n",
    "    \"Temple of Malice\",\n",
    "    \"Terminate\",\n",
    "    \"Tuktuk the Explorer\",\n",
    "    \"Vivid Marsh\",\n",
    "    \"Whip of Erebos\",\n",
    "    \"Wort, Boggart Auntie\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rogues = [\n",
    "    \"Akki Underminer\",\n",
    "    \"Amphin Pathmage\",\n",
    "    \"Aqueous Form\",\n",
    "    \"Ash Barrens\",\n",
    "    \"Ashling, the Extinguisher\",\n",
    "    \"Balefire Dragon\",\n",
    "    \"Barren Moor\",\n",
    "    \"Bident of Thassa\",\n",
    "    \"Blasphemous Act\",\n",
    "    \"Blighted Agent\",\n",
    "    \"Chromatic Lantern\",\n",
    "    \"Command Tower\",\n",
    "    \"Commander's Sphere\",\n",
    "    \"Counterspell\",\n",
    "    \"Crumbling Necropolis\",\n",
    "    \"Cyclonic Rift\",\n",
    "    \"Darkwater Catacombs\",\n",
    "    \"Decree of Pain\",\n",
    "    \"Deepchannel Mentor\",\n",
    "    \"Deepfathom Skulker\",\n",
    "    \"Diabolic Tutor\",\n",
    "    \"Dictate of Erebos\",\n",
    "    \"Dimir Aqueduct\",\n",
    "    \"Dimir Guildgate\",\n",
    "    \"Dimir Signet\",\n",
    "    \"Disallow\",\n",
    "    \"Dismal Backwater\",\n",
    "    \"Dowsing Dagger\",\n",
    "    \"Drana, Liberator of Malakir\",\n",
    "    \"Elbrus, the Binding Blade\",\n",
    "    \"Evolving Wilds\",\n",
    "    \"Exotic Orchard\",\n",
    "    \"Exsanguinate\",\n",
    "    \"Fellwar Stone\",\n",
    "    \"Filth\",\n",
    "    \"Fortune Thief\",\n",
    "    \"Goblin Vandal\",\n",
    "    \"Grave Pact\",\n",
    "    \"Halimar Depths\",\n",
    "    \"Hero's Downfall\",\n",
    "    \"Ink-Eyes, Servant of Oni\",\n",
    "    \"Island\",\n",
    "    \"Jwar Isle Refuge\",\n",
    "    \"Keeper of Keys\",\n",
    "    \"Lonely Sandbar\",\n",
    "    \"Marchesa, the Black Rose\",\n",
    "    \"Mask of Riddles\",\n",
    "    \"Master of Cruelties\",\n",
    "    \"Mind Stone\",\n",
    "    \"Mountain\",\n",
    "    \"Mu Yanling\",\n",
    "    \"Myriad Landscape\",\n",
    "    \"Mystical Tutor\",\n",
    "    \"Nicol Bolas\",\n",
    "    \"Night Market Lookout\",\n",
    "    \"Notion Thief\",\n",
    "    \"Oona's Blackguard\",\n",
    "    \"Phage the Untouchable\",\n",
    "    \"Polluted Delta\",\n",
    "    \"Pyreheart Wolf\",\n",
    "    \"Quietus Spike\",\n",
    "    \"Rakdos Guildgate\",\n",
    "    \"Rakdos Signet\",\n",
    "    \"Rankle, Master of Pranks\",\n",
    "    \"Raving Dead\",\n",
    "    \"Reliquary Tower\",\n",
    "    \"Rhystic Study\",\n",
    "    \"Rogue's Passage\",\n",
    "    \"Scion of Darkness\",\n",
    "    \"Scytheclaw\",\n",
    "    \"Sheoldred, Whispering One\",\n",
    "    \"Shizo, Death's Storehouse\",\n",
    "    \"Skeleton Key\",\n",
    "    \"Sol Ring\",\n",
    "    \"Submerged Boneyard\",\n",
    "    \"Sunken Hollow\",\n",
    "    \"Swamp\",\n",
    "    \"Sword of Sinew and Steel\",\n",
    "    \"Teleportal\",\n",
    "    \"Temple of the False God\",\n",
    "    \"Terramorphic Expanse\",\n",
    "    \"Thada Adel, Acquisitor\",\n",
    "    \"Thassa, God of the Sea\",\n",
    "    \"Thought Vessel\",\n",
    "    \"Thraximundar\",\n",
    "    \"Unclaimed Territory\",\n",
    "    \"Vraska, Scheming Gorgon\",\n",
    "    \"Whispersilk Cloak\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esper_blink = [\n",
    "    \"Acrobatic Maneuver\",\n",
    "    \"Angel of Condemnation\",\n",
    "    \"Angel of Despair\",\n",
    "    \"Angelic Chorus\",\n",
    "    \"Arcane Sanctum\",\n",
    "    \"Ashen Rider\",\n",
    "    \"Austere Command\",\n",
    "    \"Azor, the Lawbringer\",\n",
    "    \"Azorius Chancery\",\n",
    "    \"Azorius Signet\",\n",
    "    \"Baleful Strix\",\n",
    "    \"Basalt Monolith\",\n",
    "    \"Brago, King Eternal\",\n",
    "    \"Cathars' Crusade\",\n",
    "    \"Cloudblazer\",\n",
    "    \"Command Tower\",\n",
    "    \"Commander's Sphere\",\n",
    "    \"Conjurer's Closet\",\n",
    "    \"Counterspell\",\n",
    "    \"Day of Judgment\",\n",
    "    \"Deadeye Navigator\",\n",
    "    \"Dimir Signet\",\n",
    "    \"Dire Undercurrents\",\n",
    "    \"Eerie Interlude\",\n",
    "    \"Eldrazi Displacer\",\n",
    "    \"Ephara, God of the Polis\",\n",
    "    \"Esper Panorama\",\n",
    "    \"Felidar Guardian\",\n",
    "    \"Flickerform\",\n",
    "    \"Flickerwisp\",\n",
    "    \"Ghostly Flicker\",\n",
    "    \"Ghostway\",\n",
    "    \"Glacial Fortress\",\n",
    "    \"Glimmerpoint Stag\",\n",
    "    \"Gonti, Lord of Luxury\",\n",
    "    \"Halimar Depths\",\n",
    "    \"Illusionist's Stratagem\",\n",
    "    \"Island\",\n",
    "    \"Knight of the White Orchid\",\n",
    "    \"Kor Cartographer\",\n",
    "    \"Magister Sphinx\",\n",
    "    \"Merciless Eviction\",\n",
    "    \"Merieke Ri Berit\",\n",
    "    \"Mistmeadow Witch\",\n",
    "    \"Momentary Blink\",\n",
    "    \"Mulldrifter\",\n",
    "    \"Mycosynth Wellspring\",\n",
    "    \"Nebelgast Herald\",\n",
    "    \"Nephalia Smuggler\",\n",
    "    \"Orzhov Basilica\",\n",
    "    \"Orzhov Signet\",\n",
    "    \"Panharmonicon\",\n",
    "    \"Peregrine Drake\",\n",
    "    \"Plains\",\n",
    "    \"Port Town\",\n",
    "    \"Reflector Mage\",\n",
    "    \"Rescue from the Underworld\",\n",
    "    \"Rune-Scarred Demon\",\n",
    "    \"Solemn Simulacrum\",\n",
    "    \"Sphinx of the Final Word\",\n",
    "    \"Sphinx of Uthuun\",\n",
    "    \"Spine of Ish Sah\",\n",
    "    \"Stonehorn Dignitary\",\n",
    "    \"Strionic Resonator\",\n",
    "    \"Sudden Disappearance\",\n",
    "    \"Supreme Verdict\",\n",
    "    \"Suture Priest\",\n",
    "    \"Swamp\",\n",
    "    \"Temple of Deceit\",\n",
    "    \"Thought Vessel\",\n",
    "    \"Traveler's Cloak\",\n",
    "    \"Unquestioned Authority\",\n",
    "    \"Venser, Shaper Savant\",\n",
    "    \"Venser, the Sojourner\",\n",
    "    \"Wall of Omens\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_to_check = [\n",
    "    \"2XM\",\n",
    "    \"AKR\",\n",
    "    \"C20\",\n",
    "    \"CC1\",\n",
    "    \"CMC\",\n",
    "    \"CMR\",\n",
    "    \"IKO\",\n",
    "    \"JMP\",\n",
    "    \"KHC\",\n",
    "    \"M21\",\n",
    "    \"MB1\",\n",
    "    \"MH2\",\n",
    "    \"Q03\",\n",
    "    \"SLD\",\n",
    "    \"SLU\",\n",
    "    \"SS3\",\n",
    "    \"THB\",\n",
    "    \"TSR\",\n",
    "    \"ZNC\",\n",
    "    \"ZNE\",\n",
    "    \"ZNR\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = [(kykar_cards, ['W', 'U', 'R'], 'kykar.csv'),\n",
    "      (infinite_purphoros, ['R'], 'infinite_purphoros.csv'),\n",
    "      (tokens, ['W', 'G'], 'tokens.csv'),\n",
    "      (goblins, ['B', 'R'], 'goblins.csv'),\n",
    "      (rogues, ['U', 'B', 'R'], 'rogues.csv'),\n",
    "      (esper_blink, ['U', 'B', 'W'], 'esper_blink.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (deck_cards, deck_colors, f_out) in it:\n",
    "    print(f\"f_out = {f_out}\")\n",
    "    cards_to_check = (cards\n",
    "                      [cards.setname.isin(sets_to_check)\n",
    "                       & cards.colorIdentity.apply(lambda x: set(x).difference(deck_colors) == set())\n",
    "                       & ~cards.index.isin(deck_cards)]\n",
    "                      .index\n",
    "                      .unique())\n",
    "    \n",
    "    df_to_check = pd.DataFrame([{'text_a': card_text.loc[kc, 'text'],\n",
    "                                 'text_b': card_text.loc[ctc, 'text'],\n",
    "                                 'name_a': kc,\n",
    "                                 'name_b': ctc}\n",
    "                                for kc in deck_cards\n",
    "                                for ctc in cards_to_check])\n",
    "    \n",
    "    print(df_to_check.shape)\n",
    "\n",
    "    df_to_check.to_csv(os.path.join('.', f_out),\n",
    "                       index=False,\n",
    "                       quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now run the following on any machine that has those `csvs` copied to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck_names = ['esper_blink',\n",
    "              'goblins',\n",
    "              'kykar',\n",
    "              'infinite_purphoros',\n",
    "              'rogues',\n",
    "              'tokens']\n",
    "\n",
    "ds_to_check = (load_dataset('csv',\n",
    "                            data_files={k: f'{k}.csv' for k in deck_names},\n",
    "                            quoting=csv.QUOTE_ALL)\n",
    "               .map(tokenizer_map_func, batched=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "for deck_name in deck_names:\n",
    "    print(f\"deck_name = {deck_name}\")\n",
    "    p = trainer.predict(ds_to_check[deck_name])\n",
    "    print(f\"p.predictions.shape = {p.predictions.shape}\")\n",
    "\n",
    "    probs = softmax(p.predictions, axis=1)\n",
    "\n",
    "    z = pd.DataFrame({'p1': probs[:, 1],\n",
    "                      'y_pred': probs.argmax(axis=1),\n",
    "                      'name_b': ds_to_check[deck_name]['name_b'],\n",
    "                      'text_b': ds_to_check[deck_name]['text_b']})\n",
    "    z.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    recs = (z\n",
    "            .groupby(['name_b', 'text_b'])\n",
    "            .p1\n",
    "            .median()\n",
    "            .sort_values(ascending=False)\n",
    "            .reset_index())\n",
    "    \n",
    "    recs.to_parquet(f\"{deck_name}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1k = dataset['test'].select(range(1_000))\n",
    "d1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1h = dataset['test'].select(range(100))\n",
    "d1h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_preds(n=100):\n",
    "    chunk_size = 100\n",
    "    z = None\n",
    "    \n",
    "    i = 0\n",
    "    while i < n:\n",
    "        print(f\"i = {i}\")\n",
    "        d = dataset['test'][i: i + chunk_size]\n",
    "        p = (model(**{k: torch.as_tensor(np.array(v))\n",
    "                  for (k, v) in d.items()\n",
    "                  if k in ['attention_mask', 'input_ids', 'token_type_ids']})\n",
    "         [0]\n",
    "         .softmax(1))\n",
    "\n",
    "        z_now = pd.DataFrame(p.detach().numpy(), columns=['p0', 'p1'])\n",
    "        for key in ['label', 'text_a', 'text_b']:\n",
    "            z_now.loc[:, key] = d[key]\n",
    "        \n",
    "        if z is None:\n",
    "            z = z_now\n",
    "        else:\n",
    "            z = z.append(z_now, ignore_index=True)\n",
    "        \n",
    "        i += chunk_size\n",
    "        \n",
    "    z.reset_index(drop=True, inplace=True)\n",
    "    z.loc[:, 'p_delta'] = (z.p0 - z.p1).abs()\n",
    "    \n",
    "    z.loc[:, 'is_right'] = (z.p1 > z.p0) == z.label\n",
    "\n",
    "    z.sort_values(by='p_delta', inplace=True, ascending=False)\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = get_preds(500)\n",
    "\n",
    "z.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.is_right.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[~z.is_right].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_edhrec = card_text.copy()\n",
    "is_edhrec.loc[:, 'is_edhrec'] = is_edhrec.index.isin(edhrec_cards.name.unique())\n",
    "is_edhrec.reset_index(inplace=True)\n",
    "is_edhrec.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(z\n",
    " .merge(is_edhrec.rename(columns={'text': 'text_a', 'is_edhrec': 'is_edhrec_a'})[['text_a', 'is_edhrec_a']],\n",
    "        how='left',\n",
    "        on='text_a')\n",
    " .merge(is_edhrec.rename(columns={'text': 'text_b', 'is_edhrec': 'is_edhrec_b'})[['text_b', 'is_edhrec_b']],\n",
    "        how='left',\n",
    "        on='text_b')\n",
    " .groupby(['is_right', 'is_edhrec_b'])\n",
    " .is_right.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make deck predictions for one of my existing decks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# appendix\n",
    "\n",
    "the following is either hacking, didn't work, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizing sentences\n",
    "\n",
    "~~we will be reusing most of the text sentences above several times; might as well tokenize them all up front once instead of tokenizing most of them 100x later~~\n",
    "\n",
    "just do shit the way the documenation suggests we should. do them on the completely built pair parquet files below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaTokenizerFast\n",
    "# tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_tokenizer(row, *args, **kwargs):\n",
    "#     return pd.Series(tokenizer(row.text, *args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (card_text.head(20)\n",
    "#  .apply(my_tokenizer, axis=1, truncation=True, padding=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# card_text = (card_text\n",
    "#              .join(card_text\n",
    "#                    .apply(my_tokenizer, axis=1, truncation=True, padding=True)))\n",
    "\n",
    "# card_text.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### making the pair suggestions dataset\n",
    "\n",
    "okay so we have\n",
    "\n",
    "1. a train / test / val split of all cards\n",
    "1. a series of card text values (our \"sentences\")\n",
    "1. a list of `card --> deck` relationships\n",
    "\n",
    "the task now is to\n",
    "\n",
    "1. generate positive and negative cases for each card\n",
    "    + positive: `card --> deck <-- card`\n",
    "    + negative: just not that\n",
    "1. look up their text values\n",
    "1. write those values to file\n",
    "    + probably want to chunk this up somehow, maybe write 1k sentences per parquet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def card_sampler(cardlist, num_pairs_per_card=10, exclude_cards=None):\n",
    "    exclude_cards = exclude_cards or set()\n",
    "    np.random.seed(1337)\n",
    "    neg_card_options = np.array(list(all_cards.difference(exclude_cards)))\n",
    "    for card in tqdm(cardlist):\n",
    "        rec = card_text.loc[card]\n",
    "        commanders = edhrec_cards[edhrec_cards.name == card].commander.unique()\n",
    "        \n",
    "        # get positive card labels\n",
    "        pos_options = (edhrec_cards[edhrec_cards.commander.isin(commanders)\n",
    "                                    & (edhrec_cards.name != card)\n",
    "                                    & (~edhrec_cards.name.isin(exclude_cards))]\n",
    "                       .name.unique())\n",
    "\n",
    "        n_labels = min(num_pairs_per_card, pos_options.shape[0])\n",
    "        pos_cards = np.random.choice(pos_options,\n",
    "                                     size=n_labels,\n",
    "                                     replace=False)\n",
    "        \n",
    "        for pos_card in pos_cards:\n",
    "            try:\n",
    "                pos_text_rec = card_text.loc[pos_card]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    pos_text_rec = card_text.loc[pos_card.replace('//', '/')]\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            yield (*rec, *pos_text_rec, True)\n",
    "        \n",
    "        # get negative samples\n",
    "        # start by getting 3x as many as we had before, and assume that this\n",
    "        # will net at least x that weren't options before.\n",
    "        # additionally, we are comfortable having a full set of negative labels\n",
    "        # even if we had few (or no!) positive labels\n",
    "        neg_cards = np.random.choice(neg_card_options,\n",
    "                                     size=3 * num_pairs_per_card,\n",
    "                                     replace=False)\n",
    "        \n",
    "        neg_cards = list(set(neg_cards).difference(pos_cards))[:n_labels]\n",
    "        for neg_card in neg_cards:\n",
    "            neg_text_rec = card_text.loc[neg_card]\n",
    "            yield (*rec, *neg_text_rec, False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!rm -r ./data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# import os\n",
    "\n",
    "# for split_type in ['train', 'test', 'val']:\n",
    "#     for label_type in ['pos', 'neg']:\n",
    "#         os.makedirs(os.path.join('.', 'data', split_type, label_type), exist_ok=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "for split_type in ['train', 'test', 'val']:\n",
    "    os.makedirs(os.path.join('.', 'data', split_type), exist_ok=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!tree ."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import itertools\n",
    "\n",
    "def grouper(iterable, n, fillvalue=None):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return itertools.zip_longest(*args, fillvalue=fillvalue)\n",
    "\n",
    "# for g in grouper(range(20), 3):\n",
    "#     print(type(g))\n",
    "#     print(g)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# for g in grouper(card_sampler(cards_val[:3], 10), 4):\n",
    "#     for rec in g:\n",
    "#         print(rec)\n",
    "#     print()\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "NUM_PAIRS_PER_CARD = 100\n",
    "PARQUET_CHUNK_SIZE = 100_000\n",
    "\n",
    "for (split_type, cardlist, exclude_cardlist) in [['val', cards_val, None],\n",
    "                                                 ['test', cards_test, None],\n",
    "                                                 ['train', cards_train, set(cards_val).union(cards_test)], ]:\n",
    "    root = os.path.join('.', split_type)\n",
    "    sampler = card_sampler(cardlist, NUM_PAIRS_PER_CARD, exclude_cardlist)\n",
    "    for (i, grp) in enumerate(tqdm(grouper(sampler, PARQUET_CHUNK_SIZE))):\n",
    "        (pd.DataFrame(grp, columns=['text_a', 'text_b', 'label'])\n",
    "         .dropna()\n",
    "         .to_parquet(os.path.join('.', 'data', split_type, f'part-{i:0>5}.parquet')))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!tree ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the pytorch datasets\n",
    "\n",
    "basing this in large part off of [this doc page](https://huggingface.co/transformers/custom_datasets.html#nlplib)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_parquet('./data/train')\n",
    "test = pd.read_parquet('./data/test')\n",
    "val = pd.read_parquet('./data/val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### do the encodings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_encodings = tokenizer(test.text_a.values.tolist(),\n",
    "                           test.text_b.values.tolist(),\n",
    "                           truncation=True,\n",
    "                           padding=True)\n",
    "\n",
    "with open('test_encodings.pkl', 'wb') as fp:\n",
    "    pickle.dump(test_encodings, fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('test_encodings.pkl', 'rb') as fp:\n",
    "    test_encodings = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val_encodings = tokenizer(val.text_a.values.tolist(),\n",
    "                          val.text_b.values.tolist(),\n",
    "                          truncation=True,\n",
    "                          padding=True)\n",
    "\n",
    "with open('val_encodings.pkl', 'wb') as fp:\n",
    "    pickle.dump(test_encodings, fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('val_encodings.pkl', 'rb') as fp:\n",
    "    val_encodings = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[len(_) for _ in val_encodings['input_ids'][:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, the below killed the kernel... :("
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "train_encodings = tokenizer(train.text_a.values.tolist(),\n",
    "                            train.text_b.values.tolist(),\n",
    "                            truncation=True,\n",
    "                            padding=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "with open('train_encodings.pkl', 'wb') as fp:\n",
    "    pickle.dump(train_encodings, fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "with open('train_encodings.pkl', 'rb') as fp:\n",
    "    train_encodings = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "idx = 0\n",
    "rec = train.iloc[idx]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "class EdhrecDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
