{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mtg language model\n",
    "\n",
    "let's follow along with [this notebook](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb) to do a training from scratch of our MTG corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import tokenizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_LOCAL_LAPTOP = True\n",
    "TRAIN_TOKENIZER = True\n",
    "MAKE_MLM_NSP_DATASET_INPUTS = True\n",
    "MAKE_MLM_NSP_DATASET = True\n",
    "DO_PRE_TRAINING = True\n",
    "DO_SMALL_PRETRAIN_TEST = True\n",
    "\n",
    "# whether or not we cap the number of card pairs, or we choose or use\n",
    "# all possible pairs\n",
    "# NUM_MAX_PAIRS = None\n",
    "NUM_MAX_PAIRS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_TOKENIZER:\n",
    "    assert IS_LOCAL_LAPTOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS = ['train', 'test', 'validation']\n",
    "SEED = 1337\n",
    "NUM_PROC = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get raw text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_LOCAL_LAPTOP:\n",
    "    import mtg.cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache(None)\n",
    "def get_cards():\n",
    "    cards = (mtg.cards.cards_df()\n",
    "             [['name', 'multiverseId', 'scryfallId', 'type', 'manaCost',\n",
    "               'text', 'setname', 'power', 'toughness']]\n",
    "             .sort_values(by=['name', 'multiverseId'], ascending=False)\n",
    "             .groupby('name')\n",
    "             .first())\n",
    "    cards.index = cards.index.str.lower()\n",
    "    cards = cards[cards.type != 'Scheme']\n",
    "    cards.loc[:, 'mytext'] = (cards.manaCost.fillna('{0}')\n",
    "                              + ' '\n",
    "                              + cards.type\n",
    "                              + ((' ' + cards.power + '/' + cards.toughness).fillna(''))\n",
    "                              + ': '\n",
    "                              + cards.text.str.replace('\\s+', ' ').fillna(''))\n",
    "    cards.mytext = cards.mytext.str.lower().str.replace('[\"\\']', '')\n",
    "    return cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_CORPUS = 'mtg-corpus.txt'\n",
    "\n",
    "def get_corpus(f=F_CORPUS):\n",
    "    get_cards().mytext.to_csv(f, header=False, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_TOKENIZER:\n",
    "    get_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_TOKENIZER:\n",
    "    !head -n5 mtg-corpus.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train a tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found the `BERT` special tokens by looking at the defaults for\n",
    "\n",
    "```py\n",
    "tokenizers.BertWordPieceTokenizer.train?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "tokenizers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "MODEL_NAME = 'mtg-language'\n",
    "\n",
    "# if you run with vocab_size = 300_000 (default), you get 11,761 vocab items\n",
    "# any number larger than that will include null tokens and will also include\n",
    "# single words. we will choose a number that is *just* below that here\n",
    "VOCAB_SIZE = 11_500\n",
    "\n",
    "if TRAIN_TOKENIZER:\n",
    "    #trainable_tokenizer = tokenizers.ByteLevelBPETokenizer(lowercase=True)\n",
    "    trainable_tokenizer = tokenizers.BertWordPieceTokenizer(lowercase=True)\n",
    "    \n",
    "    trainable_tokenizer.train(files=F_CORPUS,\n",
    "                              vocab_size=VOCAB_SIZE,\n",
    "                              special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])\n",
    "    \n",
    "    !mkdir -p {MODEL_NAME}\n",
    "\n",
    "    trainable_tokenizer.save_model(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if TRAIN_TOKENIZER:\n",
    "    !tail -n40 {MODEL_NAME}/vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_TOKENIZER:\n",
    "    !wc -l {MODEL_NAME}/vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make a training dataset\n",
    "\n",
    "we will do pre-training of a completely un-initialized model down below, so we will need a dataset to train on to do that. `bert` has two objectives -- a masked language model (mlm) and a next sentence prediction.\n",
    "\n",
    "in order to do this, we will need to create a dataset with the following features:\n",
    "\n",
    "+ standard bert tokenizer outputs\n",
    "    + `input_ids`\n",
    "    + `attentention_mask`\n",
    "    + `token_type_ids`\n",
    "+ `labels`: _optional_, these are basically the same thing as `input_ids`, but allow for ignoring certain tokens for the purpose of loss calculations\n",
    "+ `next_sentence_label`: these are the `0, 1` values indicating whether or not sentences A and B are continuations (in the original model) or `edhrec` pairs (our model)\n",
    "\n",
    "more is better here, of course; I think the goal has to be full coverage of all cards and all edhrec pair-ups. to that end, I will create a dataset builder of a generator type.\n",
    "\n",
    "+ [writing a dataset loading script walkthrough here](https://huggingface.co/docs/datasets/add_dataset.html)\n",
    "+ [code template example here](https://github.com/huggingface/datasets/blob/master/templates/new_dataset_script.py)\n",
    "\n",
    "we have a few steps:\n",
    "\n",
    "1. create a train / test / val split of card names\n",
    "1. create a train / test / val split of edhrec pairings\n",
    "1. save the above to files we can move around (parquet is fine)\n",
    "1. create a datasetloader object that can take the above and generate the full datasets (and I do mean full!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train / test / val split for card names\n",
    "\n",
    "+ train and val are just splits on cards\n",
    "+ test is a straight holdout of a few enitre sets to test generalizability when new sets drop\n",
    "\n",
    "get test first, then do normal train/val split on the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'mtg-language-data'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_sets(target_test_frac, seed=SEED):\n",
    "    cards = get_cards()\n",
    "    mtg_set_sizes = cards.setname.value_counts()\n",
    "    mtg_sets = set(cards.setname.unique())\n",
    "\n",
    "    test_sets = set()\n",
    "    test_sets_size = 0\n",
    "    test_set_target_size = target_test_frac * cards.shape[0]\n",
    "\n",
    "    random.seed(seed)\n",
    "    while test_sets_size < test_set_target_size:\n",
    "        s = random.choice(list(mtg_sets))\n",
    "        test_sets.add(s)\n",
    "        mtg_sets.remove(s)\n",
    "        test_sets_size += mtg_set_sizes[s]\n",
    "        #print(f\"test_sets {test_sets} contain {test_sets_size} cards\")\n",
    "    return test_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_TEST_FRAC = 0.05\n",
    "VAL_FRAC = 0.05\n",
    "\n",
    "# the number of records is basically #cards ^ 2,\n",
    "# so if we ant 0.05% of all *sentence pairs* to be val, we need 0.05 ^ (1/2)% of *cards*\n",
    "# (which is about 22%)\n",
    "\n",
    "if MAKE_MLM_NSP_DATASET_INPUTS:\n",
    "    cards = get_cards()\n",
    "    \n",
    "    # if things are capped per card, number of cards is all that matters\n",
    "    # if they are not, then number of cards squared determines the set size\n",
    "    tf = TARGET_TEST_FRAC if (NUM_MAX_PAIRS is not None) else TARGET_TEST_FRAC ** .5\n",
    "    vf = VAL_FRAC if (NUM_MAX_PAIRS is not None) else VAL_FRAC ** .5\n",
    "    \n",
    "    test_sets = get_test_sets(tf)\n",
    "    is_test = cards.setname.isin(test_sets)\n",
    "    test = cards[is_test]\n",
    "    train_val = cards[~is_test]\n",
    "    \n",
    "    adj_val_frac = vf / (1 - tf)\n",
    "    if NUM_MAX_PAIRS is None:\n",
    "        adj_val_frac = adj_val_frac ** .5\n",
    "    \n",
    "    train, val = train_test_split(train_val, test_size=adj_val_frac, random_state=SEED)\n",
    "\n",
    "    print(f\"num records train: {train.shape[0]}\")\n",
    "    print(f\"num records test:  {test.shape[0]}\")\n",
    "    print(f\"num records val:   {val.shape[0]}\")\n",
    "    \n",
    "    train.to_parquet(os.path.join(DATA_DIR, \"cards.train.parquet\"), index=True)\n",
    "    test.to_parquet(os.path.join(DATA_DIR, \"cards.test.parquet\"), index=True)\n",
    "    val.to_parquet(os.path.join(DATA_DIR, \"cards.validation.parquet\"), index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train / test / val split for edhrec pairs\n",
    "\n",
    "use the card splits just defined above to subset all edhrec pairings into separate groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_MLM_NSP_DATASET_INPUTS:\n",
    "    import mtg.extract.edhrec\n",
    "\n",
    "    edhrec_cards = (mtg.extract.edhrec.get_commanders_and_cards()\n",
    "                    [['name', 'commander']])\n",
    "    commanders = edhrec_cards.commander.unique()\n",
    "    cmdr_cmdr_df = pd.DataFrame([[c, c] for c in commanders],\n",
    "                                columns=['name', 'commander'])\n",
    "    edhrec_cards = edhrec_cards.append(cmdr_cmdr_df)\n",
    "    \n",
    "    edhrec_cards.name = edhrec_cards.name.str.lower().str.replace('//', '/')\n",
    "    edhrec_cards.commander = edhrec_cards.commander.str.lower().str.replace('//', '/')\n",
    "    \n",
    "    edhrec_train = edhrec_cards[edhrec_cards.name.isin(train.index)].copy()\n",
    "    edhrec_test = edhrec_cards[edhrec_cards.name.isin(test.index)].copy()\n",
    "    edhrec_val = edhrec_cards[edhrec_cards.name.isin(val.index)].copy()\n",
    "\n",
    "    print(f\"num records train: {edhrec_train.shape[0]}\")\n",
    "    print(f\"num records test:  {edhrec_test.shape[0]}\")\n",
    "    print(f\"num records val:   {edhrec_val.shape[0]}\")\n",
    "    \n",
    "    edhrec_train.to_parquet(os.path.join(DATA_DIR, \"edhrec.train.parquet\"), index=False)\n",
    "    edhrec_test.to_parquet(os.path.join(DATA_DIR, \"edhrec.test.parquet\"), index=False)\n",
    "    edhrec_val.to_parquet(os.path.join(DATA_DIR, \"edhrec.validation.parquet\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edhrec_train.head(2) if MAKE_MLM_NSP_DATASET_INPUTS else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edhrec_test.head(2) if MAKE_MLM_NSP_DATASET_INPUTS else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the datasetbuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh {DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # quick check on the max length of our sequences:\n",
    "# # we can easily build a tokenizer and apply it to every sentence\n",
    "# # directly; this will give us a max length for a single sentence\n",
    "# # and then our dataset max length is approx double that.\n",
    "\n",
    "# from transformers import BertTokenizerFast\n",
    "\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# z = tokenizer(cards.mytext.unique().tolist(), max_length=1024)\n",
    "\n",
    "# import collections\n",
    "\n",
    "# l = [len(_) for _ in z['input_ids']]\n",
    "# c = collections.Counter(l)\n",
    "\n",
    "# df = (pd.DataFrame([{'k': k, 'v': v} for (k, v) in c.items()])\n",
    "#       .sort_values(by='k'))\n",
    "# df.loc[:, 'cs'] = df.v.cumsum() / df.v.sum()\n",
    "# print(f\"max single sequence length: {df.k.max()}\")\n",
    "# # df.plot('k', 'cs')\n",
    "\n",
    "# import numpy as np\n",
    "# pairs = np.random.choice(l, size=(100_000, 2))\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(pairs.sum(axis=1), bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "moral of the story from the above: almost every card is <100 tokens, max is 185. almost every *pair* of sequences is under 175 total tokens. 200 is *extremely* conservative actually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "from utils import build_tokenizer_map_func\n",
    "\n",
    "MAX_SEQ_LENGTH = 200\n",
    "\n",
    "if MAKE_MLM_NSP_DATASET:\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "    tokenizer_map_func = build_tokenizer_map_func(tokenizer, max_length=MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(examples):\n",
    "    return {'labels': examples['input_ids']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_MLM_NSP_DATASET:\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    \n",
    "    dataset = (datasets.load_dataset('edhrec_dataset.py', data_dir=DATA_DIR, num_max_pairs=NUM_MAX_PAIRS)\n",
    "               .shuffle(seeds={split: SEED for split in SPLITS})\n",
    "               .map(tokenizer_map_func,\n",
    "                    batched=False,\n",
    "                    num_proc=NUM_PROC)\n",
    "               .map(add_labels,\n",
    "                    batched=True,\n",
    "                    num_proc=NUM_PROC));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset if MAKE_MLM_NSP_DATASET else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape if MAKE_MLM_NSP_DATASET else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the smaller dataset defined above,\n",
    "\n",
    "+ ~~add the tokenizer~~\n",
    "+ ~~build the trainer and train config~~\n",
    "+ do a train round with the smaller datasets\n",
    "+ any difference at all??\n",
    "\n",
    "if it looks promising,\n",
    "\n",
    "+ build a *real* dataset\n",
    "    + update the 5 --> 100 or even None\n",
    "    + move to a **CPU** box -- not a GPU box. this dataset creation is done on the CPU.\n",
    "    + save this dataset and copy it down\n",
    "+ train\n",
    "    + move to GPU (upload the saved dataset\n",
    "    + **add early stopping**\n",
    "    + run it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-training\n",
    "\n",
    "we actually do care most about the nsp (next sentence prediction) task -- for us, that's the \"is edhrec pair / isn't edhrec pair\" concept. this means we *have* to do a `bert` model, because all of the other models dropped that task in favor of others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (BertConfig,\n",
    "                          BertForPreTraining,\n",
    "                          BertTokenizerFast,\n",
    "                          Trainer,\n",
    "                          TrainingArguments, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_PRE_TRAINING:\n",
    "    config = BertConfig(vocab_size=VOCAB_SIZE)\n",
    "    model = BertForPreTraining(config=config)\n",
    "    print(f'{model.num_parameters():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_SMALL_PRETRAIN_TEST:\n",
    "    TRAIN_BATCH = 8\n",
    "    EVAL_BATCH = 8\n",
    "    LOGGING_STEPS = 250\n",
    "    train_dataset = dataset['train'].select(range(80))\n",
    "    eval_dataset = dataset['validation'].select(range(80))\n",
    "else:\n",
    "    TRAIN_BATCH = 36\n",
    "    EVAL_BATCH = 36\n",
    "    LOGGING_STEPS = 250\n",
    "    train_dataset = dataset['train']\n",
    "    eval_dataset = dataset['validation']\n",
    "\n",
    "\n",
    "if DO_PRE_TRAINING:\n",
    "    output_dir = './mtg-language-results-v1'\n",
    "    \n",
    "    # # use this format to pick up from an aborted run\n",
    "    # model = BertForPreTraining.from_pretrained(f'./{output_dir}/checkpoint-5750')\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,                    # output directory\n",
    "        num_train_epochs=1,                       # total # of training epochs\n",
    "        per_device_train_batch_size=TRAIN_BATCH,  # batch size per device during training\n",
    "        per_device_eval_batch_size=EVAL_BATCH,    # batch size for evaluation\n",
    "        warmup_steps=500,                         # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,                        # strength of weight decay\n",
    "        logging_dir='./logs',                     # directory for storing logs\n",
    "        # my custom ones\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        overwrite_output_dir=True,\n",
    "        evaluation_strategy='steps',\n",
    "        logging_first_step=True,\n",
    "        seed=1337,\n",
    "        dataloader_drop_last=True,\n",
    "        dataloader_num_workers=30,\n",
    "        label_names=['labels', 'next_sentence_label'],\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,                  # the instantiated ðŸ¤— Transformers model to be trained\n",
    "        args=training_args,           # training arguments, defined above\n",
    "        train_dataset=train_dataset,  # training dataset\n",
    "        eval_dataset=eval_dataset,    # evaluation dataset\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_PRE_TRAINING:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('mtg-language-test-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    'fill-mask',\n",
    "    model='./mtg-language-test-small',\n",
    "    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['validation'][0]['text_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask('{3}{w}{b} legendary creature â€” vampire knight 4/4: vigilance, lifelink {t}, pay 7 life: destroy target nonland [MASK]. activate this ability only during your turn.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
