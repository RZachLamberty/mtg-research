{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mtg language model\n",
    "\n",
    "let's follow along with [this notebook](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb) to do a training from scratch of our MTG corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import tokenizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (BertConfig,\n",
    "                          BertForPreTraining,\n",
    "                          BertTokenizerFast,\n",
    "                          Trainer,\n",
    "                          TrainingArguments,\n",
    "                          pipeline, )\n",
    "\n",
    "from utils import grouper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_LOCAL_LAPTOP = True\n",
    "TRAIN_TOKENIZER = True\n",
    "MAKE_MLM_NSP_DATASET_INPUTS = True\n",
    "MAKE_MLM_NSP_DATASET = True\n",
    "DO_CHUNKED_DATASET_BUILDING = True\n",
    "SAVE_DATASET_TO_FILE = True\n",
    "LOAD_DATASET_FROM_FILE = False\n",
    "DO_PRE_TRAINING = False\n",
    "DO_SMALL_PRETRAIN_TEST = False\n",
    "\n",
    "# whether or not we cap the number of card pairs, or we choose or use\n",
    "# all possible pairs\n",
    "NUM_MAX_PAIRS = None\n",
    "# NUM_MAX_PAIRS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_TOKENIZER:\n",
    "    assert IS_LOCAL_LAPTOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS = ['train', 'test', 'validation']\n",
    "SEED = 1337\n",
    "\n",
    "# for the chunking of the dataset builder\n",
    "\n",
    "N_PROCS = os.cpu_count()\n",
    "N_CHUNKS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get raw text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_LOCAL_LAPTOP:\n",
    "    import mtg.cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache(None)\n",
    "def get_cards():\n",
    "    cards = (mtg.cards.cards_df()\n",
    "             [['name', 'multiverseId', 'scryfallId', 'type', 'manaCost',\n",
    "               'text', 'setname', 'power', 'toughness']]\n",
    "             .sort_values(by=['name', 'multiverseId'], ascending=False)\n",
    "             .groupby('name')\n",
    "             .first())\n",
    "    cards.index = cards.index.str.lower()\n",
    "    cards = cards[cards.type != 'Scheme']\n",
    "    cards.loc[:, 'mytext'] = (cards.manaCost.fillna('{0}')\n",
    "                              + ' '\n",
    "                              + cards.type\n",
    "                              + ((' ' + cards.power + '/' + cards.toughness).fillna(''))\n",
    "                              + ': '\n",
    "                              + cards.text.str.replace('\\s+', ' ').fillna(''))\n",
    "    cards.mytext = cards.mytext.str.lower().str.replace('[\"\\']', '')\n",
    "    return cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_CORPUS = 'mtg-corpus.txt'\n",
    "\n",
    "def get_corpus(f=F_CORPUS):\n",
    "    get_cards().mytext.to_csv(f, header=False, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_TOKENIZER:\n",
    "    get_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2}{r}{r}{g}{g} enchantment: at the beginning of your upkeep, you may say ach! hans, run! its the . . . and the name of a creature card. if you do, search your library for a card with that name, put it onto the battlefield, then shuffle your library. that creature gains haste. exile it at the beginning of the next end step.\r\n",
      "{2}{b} enchantment: {3}{b}, exile a permanent you control with a league of dastardly doom watermark: return a permanent card with a league of dastardly doom watermark from your graveyard to the battlefield.\r\n",
      "{w}{u}{b}{r}{g} summon â€” legend: cannot be the target of spells or effects. world champion has power and toughness equal to the life total of target opponent. {0}: discard your hand to search your library for 1996 world champion and reveal it to all players. shuffle your library and put 1996 world champion on top of it. use this ability only at the beginning of your upkeep, and only if 1996 world champion is in your library.\r\n",
      "{4}{w}{b} enchantment: spells and abilities you control cant destroy, exile, target, or cause you to sacrifice a good thing. at the beginning of your upkeep, double your life total. then, if you have 1,000 or more life, you lose the game.\r\n",
      "{2}{w} instant: exile target attacking creature. then remove it from the game. then put it into the absolutely-removed-from-the-freaking-game-forever zone.\r\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_TOKENIZER:\n",
    "    !head -n5 mtg-corpus.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train a tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found the `BERT` special tokens by looking at the defaults for\n",
    "\n",
    "```py\n",
    "tokenizers.BertWordPieceTokenizer.train?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.76 s, sys: 426 ms, total: 6.19 s\n",
      "Wall time: 2.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "MODEL_NAME = 'mtg-language'\n",
    "\n",
    "# if you run with vocab_size = 300_000 (default), you get 11,761 vocab items\n",
    "# any number larger than that will include null tokens and will also include\n",
    "# single words. we will choose a number that is *just* below that here\n",
    "VOCAB_SIZE = 11_500\n",
    "\n",
    "if TRAIN_TOKENIZER:\n",
    "    #trainable_tokenizer = tokenizers.ByteLevelBPETokenizer(lowercase=True)\n",
    "    trainable_tokenizer = tokenizers.BertWordPieceTokenizer(lowercase=True)\n",
    "    \n",
    "    trainable_tokenizer.train(files=F_CORPUS,\n",
    "                              vocab_size=VOCAB_SIZE,\n",
    "                              special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])\n",
    "    \n",
    "    !mkdir -p {MODEL_NAME}\n",
    "\n",
    "    trainable_tokenizer.save_model(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reliquary\r\n",
      "mediocrity\r\n",
      "petravark\r\n",
      "petrahydrox\r\n",
      "scrapheap\r\n",
      "lichenthrope\r\n",
      "ertais\r\n",
      "geargrabber\r\n",
      "unspeakable\r\n",
      "sluggishness\r\n",
      "hollowhenge\r\n",
      "belzenlok\r\n",
      "cerulean\r\n",
      "vizkopa\r\n",
      "vitaspore\r\n",
      "stingmoggie\r\n",
      "miscreant\r\n",
      "misfortune\r\n",
      "saltskitter\r\n",
      "saltcrusted\r\n",
      "comeuppance\r\n",
      "collaborator\r\n",
      "eddytrail\r\n",
      "prognostic\r\n",
      "polyraptor\r\n",
      "morselhoarder\r\n",
      "hemorrhage\r\n",
      "melancholy\r\n",
      "pummeler\r\n",
      "tromokratis\r\n",
      "uncontested\r\n",
      "thraximundar\r\n",
      "astrolabe\r\n",
      "bangchuckers\r\n",
      "epochrasite\r\n",
      "gurzigost\r\n",
      "kaleidostone\r\n",
      "moondrakes\r\n",
      "nucklavee\r\n",
      "svyelunite\r\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_TOKENIZER:\n",
    "    !tail -n40 {MODEL_NAME}/vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   11400 mtg-language/vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_TOKENIZER:\n",
    "    !wc -l {MODEL_NAME}/vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make a training dataset\n",
    "\n",
    "we will do pre-training of a completely un-initialized model down below, so we will need a dataset to train on to do that. `bert` has two objectives -- a masked language model (mlm) and a next sentence prediction.\n",
    "\n",
    "in order to do this, we will need to create a dataset with the following features:\n",
    "\n",
    "+ standard bert tokenizer outputs\n",
    "    + `input_ids`\n",
    "    + `attentention_mask`\n",
    "    + `token_type_ids`\n",
    "+ `labels`: _optional_, these are basically the same thing as `input_ids`, but allow for ignoring certain tokens for the purpose of loss calculations\n",
    "+ `next_sentence_label`: these are the `0, 1` values indicating whether or not sentences A and B are continuations (in the original model) or `edhrec` pairs (our model)\n",
    "\n",
    "more is better here, of course; I think the goal has to be full coverage of all cards and all edhrec pair-ups. to that end, I will create a dataset builder of a generator type.\n",
    "\n",
    "+ [writing a dataset loading script walkthrough here](https://huggingface.co/docs/datasets/add_dataset.html)\n",
    "+ [code template example here](https://github.com/huggingface/datasets/blob/master/templates/new_dataset_script.py)\n",
    "\n",
    "we have a few steps:\n",
    "\n",
    "1. create a train / test / val split of card names\n",
    "1. create a train / test / val split of edhrec pairings\n",
    "1. save the above to files we can move around (parquet is fine)\n",
    "1. create a datasetloader object that can take the above and generate the full datasets (and I do mean full!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train / test / val split for card names\n",
    "\n",
    "+ train and val are just splits on cards\n",
    "+ test is a straight holdout of a few enitre sets to test generalizability when new sets drop\n",
    "\n",
    "get test first, then do normal train/val split on the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'mtg-language-data'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_sets(target_test_frac, seed=SEED):\n",
    "    cards = get_cards()\n",
    "    mtg_set_sizes = cards.setname.value_counts()\n",
    "    mtg_sets = set(cards.setname.unique())\n",
    "\n",
    "    test_sets = set()\n",
    "    test_sets_size = 0\n",
    "    test_set_target_size = target_test_frac * cards.shape[0]\n",
    "\n",
    "    random.seed(seed)\n",
    "    while test_sets_size < test_set_target_size:\n",
    "        s = random.choice(list(mtg_sets))\n",
    "        test_sets.add(s)\n",
    "        mtg_sets.remove(s)\n",
    "        test_sets_size += mtg_set_sizes[s]\n",
    "        #print(f\"test_sets {test_sets} contain {test_sets_size} cards\")\n",
    "    return test_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to create three datasets (`train`, `test`, `validation`) that are approximately 0.9, 0.05, 0.05. there are two ways we generate records for that dataset, though:\n",
    "\n",
    "+ a fixed number (e.g. 100) of pairs per card\n",
    "    + e.g. take card a and generate up to 100 true / false high / false low pairs\n",
    "+ as many pairs per card as are allowed under the provided card split\n",
    "    + e.g. combine every card with every other card and also provide a 0/1 label based on whether or not it was an edhrec pair\n",
    "\n",
    "we want to create these *record-level* splits by making *card-level* splits first. these two different end results require two different splitting fractions for the cards:\n",
    "\n",
    "+ for option 1 (fixed pairs per card), the number of *records* per *card set* is N * num_cards\n",
    "+ for option 2 (*not* capped), the number of *records* per *card set* is roughly num_cards ^ 2\n",
    "\n",
    "option 1 is easy (just do .9/.05/.05). option 2 is trickier. we need $\\alpha + 2\\beta = 1$, and also want $\\dfrac{\\alpha ^ 2}{\\beta ^ 2} \\approx \\dfrac{0.9}{0.05}$, i.e. $\\alpha ^ 2 \\approx 18 \\beta ^ 2$\n",
    "\n",
    "together, this means\n",
    "\n",
    "$$\n",
    "\\alpha = 1 - 2 \\beta \\\\\n",
    "\\alpha ^ 2 = 1 - 4 \\beta + 4 \\beta ^ 2 \\\\\n",
    "1 - 4 \\beta + 4 \\beta ^ 2 \\approx 18 \\beta ^ 2 \\\\\n",
    "1 - 4 \\beta - 14 \\beta ^ 2 \\approx 0 \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16018862050852034"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b, c = -14, -4, 1\n",
    "beta = (-b - (b ** 2 - 4 * a * c) ** .5) / (2 * a)\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num records train: 14769\n",
      "num records test:  3501\n",
      "num records val:   3486\n"
     ]
    }
   ],
   "source": [
    "if NUM_MAX_PAIRS is None:\n",
    "    TARGET_TEST_FRAC = beta\n",
    "    VAL_FRAC = beta\n",
    "else:\n",
    "    TARGET_TEST_FRAC = 0.05\n",
    "    VAL_FRAC = 0.05\n",
    "\n",
    "if MAKE_MLM_NSP_DATASET_INPUTS:\n",
    "    cards = get_cards()\n",
    "    \n",
    "    test_sets = get_test_sets(TARGET_TEST_FRAC)\n",
    "    is_test = cards.setname.isin(test_sets)\n",
    "    test = cards[is_test]\n",
    "    train_val = cards[~is_test]\n",
    "    \n",
    "    # we want val frac relative to the *entire* dataset, which means we need to\n",
    "    # scale it up when part of that dataset has been claimed by the test set\n",
    "    # in other words, we want n_val records, where n_val = val_frac * N_total\n",
    "    # now that n_test have been removed,\n",
    "    # \n",
    "    #   n_val = adj_val_frac * (N_total - n_test)\n",
    "    #   adj_val_frac = n_val / (N_total - n_test)\n",
    "    #   adj_val_frac = val_frac * N_total / (N_total - n_test)\n",
    "    adj_val_frac = VAL_FRAC * cards.shape[0] / train_val.shape[0]\n",
    "    \n",
    "    train, val = train_test_split(train_val, test_size=adj_val_frac, random_state=SEED)\n",
    "\n",
    "    print(f\"num records train: {train.shape[0]}\")\n",
    "    print(f\"num records test:  {test.shape[0]}\")\n",
    "    print(f\"num records val:   {val.shape[0]}\")\n",
    "    \n",
    "    train.to_parquet(os.path.join(DATA_DIR, \"cards.train.parquet\"), index=True)\n",
    "    test.to_parquet(os.path.join(DATA_DIR, \"cards.test.parquet\"), index=True)\n",
    "    val.to_parquet(os.path.join(DATA_DIR, \"cards.validation.parquet\"), index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train / test / val split for edhrec pairs\n",
    "\n",
    "use the card splits just defined above to subset all edhrec pairings into separate groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num records train: 153021\n",
      "num records test:  56685\n",
      "num records val:   30442\n"
     ]
    }
   ],
   "source": [
    "if MAKE_MLM_NSP_DATASET_INPUTS:\n",
    "    import mtg.extract.edhrec\n",
    "\n",
    "    edhrec_cards = (mtg.extract.edhrec.get_commanders_and_cards()\n",
    "                    [['name', 'commander']])\n",
    "    commanders = edhrec_cards.commander.unique()\n",
    "    cmdr_cmdr_df = pd.DataFrame([[c, c] for c in commanders],\n",
    "                                columns=['name', 'commander'])\n",
    "    edhrec_cards = edhrec_cards.append(cmdr_cmdr_df)\n",
    "    \n",
    "    edhrec_cards.name = edhrec_cards.name.str.lower().str.replace('//', '/')\n",
    "    edhrec_cards.commander = edhrec_cards.commander.str.lower().str.replace('//', '/')\n",
    "    \n",
    "    edhrec_train = edhrec_cards[edhrec_cards.name.isin(train.index)].copy()\n",
    "    edhrec_test = edhrec_cards[edhrec_cards.name.isin(test.index)].copy()\n",
    "    edhrec_val = edhrec_cards[edhrec_cards.name.isin(val.index)].copy()\n",
    "\n",
    "    print(f\"num records train: {edhrec_train.shape[0]}\")\n",
    "    print(f\"num records test:  {edhrec_test.shape[0]}\")\n",
    "    print(f\"num records val:   {edhrec_val.shape[0]}\")\n",
    "    \n",
    "    edhrec_train.to_parquet(os.path.join(DATA_DIR, \"edhrec.train.parquet\"), index=False)\n",
    "    edhrec_test.to_parquet(os.path.join(DATA_DIR, \"edhrec.test.parquet\"), index=False)\n",
    "    edhrec_val.to_parquet(os.path.join(DATA_DIR, \"edhrec.validation.parquet\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>commander</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>forsaken monument</td>\n",
       "      <td>kozilek, the great distortion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>war room</td>\n",
       "      <td>kozilek, the great distortion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name                      commander\n",
       "0  forsaken monument  kozilek, the great distortion\n",
       "1           war room  kozilek, the great distortion"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edhrec_train.head(2) if MAKE_MLM_NSP_DATASET_INPUTS else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>commander</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hedron archive</td>\n",
       "      <td>kozilek, the great distortion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mind stone</td>\n",
       "      <td>kozilek, the great distortion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name                      commander\n",
       "7   hedron archive  kozilek, the great distortion\n",
       "10      mind stone  kozilek, the great distortion"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edhrec_test.head(2) if MAKE_MLM_NSP_DATASET_INPUTS else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the datasetbuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh {DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # quick check on the max length of our sequences:\n",
    "# # we can easily build a tokenizer and apply it to every sentence\n",
    "# # directly; this will give us a max length for a single sentence\n",
    "# # and then our dataset max length is approx double that.\n",
    "\n",
    "# from transformers import BertTokenizerFast\n",
    "\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# z = tokenizer(cards.mytext.unique().tolist(), max_length=1024)\n",
    "\n",
    "# import collections\n",
    "\n",
    "# l = [len(_) for _ in z['input_ids']]\n",
    "# c = collections.Counter(l)\n",
    "\n",
    "# df = (pd.DataFrame([{'k': k, 'v': v} for (k, v) in c.items()])\n",
    "#       .sort_values(by='k'))\n",
    "# df.loc[:, 'cs'] = df.v.cumsum() / df.v.sum()\n",
    "# print(f\"max single sequence length: {df.k.max()}\")\n",
    "# # df.plot('k', 'cs')\n",
    "\n",
    "# import numpy as np\n",
    "# pairs = np.random.choice(l, size=(100_000, 2))\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(pairs.sum(axis=1), bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "moral of the story from the above: almost every card is <100 tokens, max is 185. almost every *pair* of sequences is under 175 total tokens. 200 is *extremely* conservative actually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 200\n",
    "\n",
    "if MAKE_MLM_NSP_DATASET:\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "    tokenizer_map_func = build_tokenizer_map_func(tokenizer, max_length=MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(examples):\n",
    "    return {'labels': examples['input_ids']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_MLM_NSP_DATASET:\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    \n",
    "    if DO_CHUNKED_DATASET_BUILDING:\n",
    "        commands = [\n",
    "            f'python edhrec_dataset_chunk_proc.py -n {N_CHUNKS} -c {c} -d mtg-language-data -o mtg-mlm-nsp-dataset-chunks > log.{c}.txt 2>&1'\n",
    "            for c in range(N_CHUNKS)\n",
    "        ]\n",
    "\n",
    "        print(f'executing {N_CHUNKS} commands in chunks of {N_PROCS} parallel commands')\n",
    "\n",
    "        for cmd_grp in grouper(tqdm(commands), N_PROCS, ''):\n",
    "            processes = [subprocess.Popen(cmd, shell=True) for cmd in cmd_grp]\n",
    "            for p in processes:\n",
    "                p.wait()\n",
    "\n",
    "            # we just saved 5 chunks in two places -- the hf cache directory\n",
    "            # and the local directory. to avoid running out of disk space, we\n",
    "            # will wipe the cache directory after every proc group\n",
    "            hf_cache_dir = os.path.join(os.path.expanduser('~'), '.cache', 'huggingface', 'datasets')\n",
    "            if os.path.isdir(hf_cache_dir):\n",
    "                os.rmdir(hf_cache_dir)\n",
    "\n",
    "        base_dataset = datasets.DatasetDict({\n",
    "            split: concatenate_datasets([load_from_disk(f)\n",
    "                                         for f in glob.glob('mtg-mlm-nsp-dataset-chunks/*')])\n",
    "            for split in SPLITS})\n",
    "    else:\n",
    "        base_dataset = datasets.load_dataset('edhrec_dataset.py',\n",
    "                                             data_dir=DATA_DIR,\n",
    "                                             num_max_pairs=NUM_MAX_PAIRS)\n",
    "\n",
    "    dataset = (base_dataset\n",
    "               .shuffle(seeds={split: SEED for split in SPLITS})\n",
    "               .map(tokenizer_map_func,\n",
    "                    batched=False,\n",
    "                    num_proc=NUM_PROC)\n",
    "               .map(add_labels,\n",
    "                    batched=True,\n",
    "                    num_proc=NUM_PROC));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset if MAKE_MLM_NSP_DATASET else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape if MAKE_MLM_NSP_DATASET else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_DATASET_TO_FILE:\n",
    "    dataset.save_to_disk('mtg-mlm-nsp-dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the smaller dataset defined above,\n",
    "\n",
    "+ ~~add the tokenizer~~\n",
    "+ ~~build the trainer and train config~~\n",
    "+ ~~do a train round with the smaller datasets~~\n",
    "+ ~~any difference at all??~~\n",
    "\n",
    "if it looks promising,\n",
    "\n",
    "+ build a *real* dataset\n",
    "    + ~~write chunked dataset processer to leverage multiple cpus~~\n",
    "    + move to a **CPU** box -- not a GPU box. this dataset creation is done on the CPU.\n",
    "    + set up cpu box\n",
    "        + `ssh-keygen -t ed25519 -C \"r.zach.lamberty@gmail.com\"`\n",
    "        + `cat ~/.ssh/id_ed25519`\n",
    "        + add to github [here](https://github.com/settings/keys)\n",
    "        + `git clone git@github.com:RZachLamberty/mtg-research.git`\n",
    "        + `source activate ...`\n",
    "        + `pip install datasets transformers`\n",
    "        + `jupyter notebook --no-browser --ip=0.0.0.0`\n",
    "    + locally\n",
    "        + `ssh -NfL 9999:localhost:8888 rzlcpu`\n",
    "        + https://localhost:9999\n",
    "        + `scp -r mtg-language rzlcpu:~/mtg-research/bert-edh-pair-prediction/mtg-language`\n",
    "        + `scp -r mtg-language-data rzlcpu:~/mtg-research/bert-edh-pair-prediction/mtg-language-data`\n",
    "    + in the jupyter notebook, update the flags at the top\n",
    "        + `True`:\n",
    "            + `MAKE_MLM_NSP_DATASET`\n",
    "            + `DO_CHUNKED_DATASET_BUILDING`\n",
    "        + all others `False`\n",
    "    + save this dataset and copy it down\n",
    "+ train\n",
    "    + move to GPU (upload the saved dataset\n",
    "    + **add early stopping**\n",
    "    + run it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-training\n",
    "\n",
    "we actually do care most about the nsp (next sentence prediction) task -- for us, that's the \"is edhrec pair / isn't edhrec pair\" concept. this means we *have* to do a `bert` model, because all of the other models dropped that task in favor of others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_PRE_TRAINING:\n",
    "    config = BertConfig(vocab_size=VOCAB_SIZE)\n",
    "    model = BertForPreTraining(config=config)\n",
    "    print(f'{model.num_parameters():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_SMALL_PRETRAIN_TEST:\n",
    "    TRAIN_BATCH = 8\n",
    "    EVAL_BATCH = 8\n",
    "    LOGGING_STEPS = 250\n",
    "    train_dataset = dataset['train'].select(range(80))\n",
    "    eval_dataset = dataset['validation'].select(range(80))\n",
    "else:\n",
    "    TRAIN_BATCH = 36\n",
    "    EVAL_BATCH = 36\n",
    "    LOGGING_STEPS = 250\n",
    "    train_dataset = dataset['train']\n",
    "    eval_dataset = dataset['validation']\n",
    "\n",
    "\n",
    "if DO_PRE_TRAINING:\n",
    "    output_dir = './mtg-language-results-v1'\n",
    "    \n",
    "    # # use this format to pick up from an aborted run\n",
    "    # model = BertForPreTraining.from_pretrained(f'./{output_dir}/checkpoint-5750')\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,                    # output directory\n",
    "        num_train_epochs=1,                       # total # of training epochs\n",
    "        per_device_train_batch_size=TRAIN_BATCH,  # batch size per device during training\n",
    "        per_device_eval_batch_size=EVAL_BATCH,    # batch size for evaluation\n",
    "        warmup_steps=500,                         # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,                        # strength of weight decay\n",
    "        logging_dir='./logs',                     # directory for storing logs\n",
    "        # my custom ones\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        overwrite_output_dir=True,\n",
    "        evaluation_strategy='steps',\n",
    "        logging_first_step=True,\n",
    "        seed=1337,\n",
    "        dataloader_drop_last=True,\n",
    "        dataloader_num_workers=30,\n",
    "        label_names=['labels', 'next_sentence_label'],\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,                  # the instantiated ðŸ¤— Transformers model to be trained\n",
    "        args=training_args,           # training arguments, defined above\n",
    "        train_dataset=train_dataset,  # training dataset\n",
    "        eval_dataset=eval_dataset,    # evaluation dataset\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_PRE_TRAINING:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('mtg-language-test-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\n",
    "    'fill-mask',\n",
    "    model='./mtg-language-test-small',\n",
    "    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['validation'][0]['text_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask('{3}{w}{b} legendary creature â€” vampire knight 4/4: vigilance, lifelink {t}, pay 7 life: destroy target nonland [MASK]. activate this ability only during your turn.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
