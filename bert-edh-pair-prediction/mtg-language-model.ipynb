{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mtg language model\n",
    "\n",
    "let's follow along with [this notebook](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb) to do a training from scratch of our MTG corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import tokenizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (BertConfig,\n",
    "                          BertForPreTraining,\n",
    "                          BertTokenizerFast,\n",
    "                          Trainer,\n",
    "                          TrainingArguments,\n",
    "                          pipeline, )\n",
    "\n",
    "from utils import build_tokenizer_map_func, grouper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_LOCAL_LAPTOP = False\n",
    "TRAIN_TOKENIZER = False\n",
    "MAKE_MLM_NSP_DATASET_INPUTS = False\n",
    "MAKE_MLM_NSP_DATASET = True\n",
    "DO_CHUNKED_DATASET_BUILDING = True\n",
    "SAVE_DATASET_TO_FILE = True\n",
    "LOAD_DATASET_FROM_FILE = False\n",
    "DO_PRE_TRAINING = False\n",
    "DO_SMALL_PRETRAIN_TEST = False\n",
    "\n",
    "# whether or not we cap the number of card pairs, or we choose or use\n",
    "# all possible pairs\n",
    "# NUM_MAX_PAIRS = None\n",
    "NUM_MAX_PAIRS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_TOKENIZER:\n",
    "    assert IS_LOCAL_LAPTOP\n",
    "    \n",
    "if DO_CHUNKED_DATASET_BUILDING:\n",
    "    assert NUM_MAX_PAIRS is not None, \"this is like 2 TB of data\"",
    "\n",
    "if MAKE_MLM_NSP_DATASET_INPUTS:\n",
    "    assert IS_LOCAL_LAPTOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS = ['train', 'test', 'validation']\n",
    "SEED = 1337\n",
    "\n",
    "# for the chunking of the dataset builder\n",
    "\n",
    "N_PROCS = os.cpu_count()\n",
    "N_CHUNKS = N_PROCS * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get raw text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_LOCAL_LAPTOP:\n",
    "    import mtg.cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache(None)\n",
    "def get_cards():\n",
    "    cards = (mtg.cards.cards_df()\n",
    "             [['name', 'multiverseId', 'scryfallId', 'type', 'manaCost',\n",
    "               'text', 'setname', 'power', 'toughness']]\n",
    "             .sort_values(by=['name', 'multiverseId'], ascending=False)\n",
    "             .groupby('name')\n",
    "             .first())\n",
    "    cards.index = cards.index.str.lower()\n",
    "    cards = cards[cards.type != 'Scheme']\n",
    "    cards.loc[:, 'mytext'] = (cards.manaCost.fillna('{0}')\n",
    "                              + ' '\n",
    "                              + cards.type\n",
    "                              + ((' ' + cards.power + '/' + cards.toughness).fillna(''))\n",
    "                              + ': '\n",
    "                              + cards.text.str.replace('\\s+', ' ').fillna(''))\n",
    "    cards.mytext = cards.mytext.str.lower().str.replace('[\"\\']', '')\n",
    "    return cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_CORPUS = 'mtg-corpus.txt'\n",
    "\n",
    "def get_corpus(f=F_CORPUS):\n",
    "    get_cards().mytext.to_csv(f, header=False, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_TOKENIZER:\n",
    "    get_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_TOKENIZER:\n",
    "    !head -n5 mtg-corpus.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train a tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found the `BERT` special tokens by looking at the defaults for\n",
    "\n",
    "```py\n",
    "tokenizers.BertWordPieceTokenizer.train?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "MODEL_NAME = 'mtg-language'\n",
    "\n",
    "# if you run with vocab_size = 300_000 (default), you get 11,761 vocab items\n",
    "# any number larger than that will include null tokens and will also include\n",
    "# single words. we will choose a number that is *just* below that here\n",
    "VOCAB_SIZE = 11_500\n",
    "\n",
    "if TRAIN_TOKENIZER:\n",
    "    #trainable_tokenizer = tokenizers.ByteLevelBPETokenizer(lowercase=True)\n",
    "    trainable_tokenizer = tokenizers.BertWordPieceTokenizer(lowercase=True)\n",
    "    \n",
    "    trainable_tokenizer.train(files=F_CORPUS,\n",
    "                              vocab_size=VOCAB_SIZE,\n",
    "                              special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])\n",
    "    \n",
    "    !mkdir -p {MODEL_NAME}\n",
    "\n",
    "    trainable_tokenizer.save_model(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if TRAIN_TOKENIZER:\n",
    "    !tail -n40 {MODEL_NAME}/vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_TOKENIZER:\n",
    "    !wc -l {MODEL_NAME}/vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make a training dataset\n",
    "\n",
    "we will do pre-training of a completely un-initialized model down below, so we will need a dataset to train on to do that. `bert` has two objectives -- a masked language model (mlm) and a next sentence prediction.\n",
    "\n",
    "in order to do this, we will need to create a dataset with the following features:\n",
    "\n",
    "+ standard bert tokenizer outputs\n",
    "    + `input_ids`\n",
    "    + `attentention_mask`\n",
    "    + `token_type_ids`\n",
    "+ `labels`: _optional_, these are basically the same thing as `input_ids`, but allow for ignoring certain tokens for the purpose of loss calculations\n",
    "+ `next_sentence_label`: these are the `0, 1` values indicating whether or not sentences A and B are continuations (in the original model) or `edhrec` pairs (our model)\n",
    "\n",
    "more is better here, of course; I think the goal has to be full coverage of all cards and all edhrec pair-ups. to that end, I will create a dataset builder of a generator type.\n",
    "\n",
    "+ [writing a dataset loading script walkthrough here](https://huggingface.co/docs/datasets/add_dataset.html)\n",
    "+ [code template example here](https://github.com/huggingface/datasets/blob/master/templates/new_dataset_script.py)\n",
    "\n",
    "we have a few steps:\n",
    "\n",
    "1. create a train / test / val split of card names\n",
    "1. create a train / test / val split of edhrec pairings\n",
    "1. save the above to files we can move around (parquet is fine)\n",
    "1. create a datasetloader object that can take the above and generate the full datasets (and I do mean full!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train / test / val split for card names\n",
    "\n",
    "+ train and val are just splits on cards\n",
    "+ test is a straight holdout of a few enitre sets to test generalizability when new sets drop\n",
    "\n",
    "get test first, then do normal train/val split on the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'mtg-language-data'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_sets(target_test_frac, seed=SEED):\n",
    "    cards = get_cards()\n",
    "    mtg_set_sizes = cards.setname.value_counts()\n",
    "    mtg_sets = set(cards.setname.unique())\n",
    "\n",
    "    test_sets = set()\n",
    "    test_sets_size = 0\n",
    "    test_set_target_size = target_test_frac * cards.shape[0]\n",
    "\n",
    "    random.seed(seed)\n",
    "    while test_sets_size < test_set_target_size:\n",
    "        s = random.choice(list(mtg_sets))\n",
    "        test_sets.add(s)\n",
    "        mtg_sets.remove(s)\n",
    "        test_sets_size += mtg_set_sizes[s]\n",
    "        #print(f\"test_sets {test_sets} contain {test_sets_size} cards\")\n",
    "    return test_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to create three datasets (`train`, `test`, `validation`) that are approximately 0.9, 0.05, 0.05. there are two ways we generate records for that dataset, though:\n",
    "\n",
    "+ a fixed number (e.g. 100) of pairs per card\n",
    "    + e.g. take card a and generate up to 100 true / false high / false low pairs\n",
    "+ as many pairs per card as are allowed under the provided card split\n",
    "    + e.g. combine every card with every other card and also provide a 0/1 label based on whether or not it was an edhrec pair\n",
    "\n",
    "we want to create these *record-level* splits by making *card-level* splits first. these two different end results require two different splitting fractions for the cards:\n",
    "\n",
    "+ for option 1 (fixed pairs per card), the number of *records* per *card set* is N * num_cards\n",
    "+ for option 2 (*not* capped), the number of *records* per *card set* is roughly num_cards ^ 2\n",
    "\n",
    "option 1 is easy (just do .9/.05/.05). option 2 is trickier. we need $\\alpha + 2\\beta = 1$, and also want $\\dfrac{\\alpha ^ 2}{\\beta ^ 2} \\approx \\dfrac{0.9}{0.05}$, i.e. $\\alpha ^ 2 \\approx 18 \\beta ^ 2$\n",
    "\n",
    "together, this means\n",
    "\n",
    "$$\n",
    "\\alpha = 1 - 2 \\beta \\\\\n",
    "\\alpha ^ 2 = 1 - 4 \\beta + 4 \\beta ^ 2 \\\\\n",
    "1 - 4 \\beta + 4 \\beta ^ 2 \\approx 18 \\beta ^ 2 \\\\\n",
    "1 - 4 \\beta - 14 \\beta ^ 2 \\approx 0 \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = -14, -4, 1\n",
    "beta = (-b - (b ** 2 - 4 * a * c) ** .5) / (2 * a)\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NUM_MAX_PAIRS is None:\n",
    "    TARGET_TEST_FRAC = beta\n",
    "    VAL_FRAC = beta\n",
    "else:\n",
    "    TARGET_TEST_FRAC = 0.05\n",
    "    VAL_FRAC = 0.05\n",
    "\n",
    "if MAKE_MLM_NSP_DATASET_INPUTS:\n",
    "    cards = get_cards()\n",
    "    \n",
    "    test_sets = get_test_sets(TARGET_TEST_FRAC)\n",
    "    is_test = cards.setname.isin(test_sets)\n",
    "    test = cards[is_test]\n",
    "    train_val = cards[~is_test]\n",
    "    \n",
    "    # we want val frac relative to the *entire* dataset, which means we need to\n",
    "    # scale it up when part of that dataset has been claimed by the test set\n",
    "    # in other words, we want n_val records, where n_val = val_frac * N_total\n",
    "    # now that n_test have been removed,\n",
    "    # \n",
    "    #   n_val = adj_val_frac * (N_total - n_test)\n",
    "    #   adj_val_frac = n_val / (N_total - n_test)\n",
    "    #   adj_val_frac = val_frac * N_total / (N_total - n_test)\n",
    "    adj_val_frac = VAL_FRAC * cards.shape[0] / train_val.shape[0]\n",
    "    \n",
    "    train, val = train_test_split(train_val, test_size=adj_val_frac, random_state=SEED)\n",
    "\n",
    "    print(f\"num records train: {train.shape[0]}\")\n",
    "    print(f\"num records test:  {test.shape[0]}\")\n",
    "    print(f\"num records val:   {val.shape[0]}\")\n",
    "    \n",
    "    train.to_parquet(os.path.join(DATA_DIR, \"cards.train.parquet\"), index=True)\n",
    "    test.to_parquet(os.path.join(DATA_DIR, \"cards.test.parquet\"), index=True)\n",
    "    val.to_parquet(os.path.join(DATA_DIR, \"cards.validation.parquet\"), index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train / test / val split for edhrec pairs\n",
    "\n",
    "use the card splits just defined above to subset all edhrec pairings into separate groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_MLM_NSP_DATASET_INPUTS:\n",
    "    import mtg.extract.edhrec\n",
    "\n",
    "    edhrec_cards = (mtg.extract.edhrec.get_commanders_and_cards()\n",
    "                    [['name', 'commander']])\n",
    "    commanders = edhrec_cards.commander.unique()\n",
    "    cmdr_cmdr_df = pd.DataFrame([[c, c] for c in commanders],\n",
    "                                columns=['name', 'commander'])\n",
    "    edhrec_cards = edhrec_cards.append(cmdr_cmdr_df)\n",
    "    \n",
    "    edhrec_cards.name = edhrec_cards.name.str.lower().str.replace('//', '/')\n",
    "    edhrec_cards.commander = edhrec_cards.commander.str.lower().str.replace('//', '/')\n",
    "    \n",
    "    edhrec_train = edhrec_cards[edhrec_cards.name.isin(train.index)].copy()\n",
    "    edhrec_test = edhrec_cards[edhrec_cards.name.isin(test.index)].copy()\n",
    "    edhrec_val = edhrec_cards[edhrec_cards.name.isin(val.index)].copy()\n",
    "\n",
    "    print(f\"num records train: {edhrec_train.shape[0]}\")\n",
    "    print(f\"num records test:  {edhrec_test.shape[0]}\")\n",
    "    print(f\"num records val:   {edhrec_val.shape[0]}\")\n",
    "    \n",
    "    edhrec_train.to_parquet(os.path.join(DATA_DIR, \"edhrec.train.parquet\"), index=False)\n",
    "    edhrec_test.to_parquet(os.path.join(DATA_DIR, \"edhrec.test.parquet\"), index=False)\n",
    "    edhrec_val.to_parquet(os.path.join(DATA_DIR, \"edhrec.validation.parquet\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edhrec_train.head(2) if MAKE_MLM_NSP_DATASET_INPUTS else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edhrec_test.head(2) if MAKE_MLM_NSP_DATASET_INPUTS else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the datasetbuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh {DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # quick check on the max length of our sequences:\n",
    "# # we can easily build a tokenizer and apply it to every sentence\n",
    "# # directly; this will give us a max length for a single sentence\n",
    "# # and then our dataset max length is approx double that.\n",
    "\n",
    "# from transformers import BertTokenizerFast\n",
    "\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# z = tokenizer(cards.mytext.unique().tolist(), max_length=1024)\n",
    "\n",
    "# import collections\n",
    "\n",
    "# l = [len(_) for _ in z['input_ids']]\n",
    "# c = collections.Counter(l)\n",
    "\n",
    "# df = (pd.DataFrame([{'k': k, 'v': v} for (k, v) in c.items()])\n",
    "#       .sort_values(by='k'))\n",
    "# df.loc[:, 'cs'] = df.v.cumsum() / df.v.sum()\n",
    "# print(f\"max single sequence length: {df.k.max()}\")\n",
    "# # df.plot('k', 'cs')\n",
    "\n",
    "# import numpy as np\n",
    "# pairs = np.random.choice(l, size=(100_000, 2))\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(pairs.sum(axis=1), bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "moral of the story from the above: almost every card is <100 tokens, max is 185. almost every *pair* of sequences is under 175 total tokens. 200 is *extremely* conservative actually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(examples):\n",
    "    return {'labels': examples['input_ids']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chunked_dataset_shell_cmd(n_chunks, c, num_max_pairs, max_seq_length):\n",
    "    nmp_flag = f' -m {num_max_pairs}' if num_max_pairs is not None else ''\n",
    "    return (f'python edhrec_dataset_chunk_proc.py'\n",
    "            f' -n {N_CHUNKS}'\n",
    "            f' -c {c}'\n",
    "            f' -d mtg-language-data'\n",
    "            f' -o mtg-mlm-nsp-dataset-chunks'\n",
    "            f'{nmp_flag}'\n",
    "            f' -l {max_seq_length}'\n",
    "            f' > logs/log.{c}.txt 2>&1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the logs and outputs\n",
    "if DO_CHUNKED_DATASET_BUILDING:\n",
    "    shutil.rmtree('logs', ignore_errors=True)\n",
    "    shutil.rmtree('mtg-mlm-nsp-dataset-chunks/', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 200\n",
    "\n",
    "if MAKE_MLM_NSP_DATASET:\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    \n",
    "    for split in SPLITS:\n",
    "        n = pd.read_parquet(os.path.join(DATA_DIR, f'cards.{split}.parquet')).shape[0]\n",
    "        print(f\"num records {split}: {n}\")\n",
    "    \n",
    "    if DO_CHUNKED_DATASET_BUILDING:\n",
    "        if not os.path.isdir('logs'):\n",
    "            os.makedirs('logs')\n",
    "\n",
    "        commands = [make_chunked_dataset_shell_cmd(N_CHUNKS, c, NUM_MAX_PAIRS, MAX_SEQ_LENGTH)\n",
    "                    for c in range(N_CHUNKS)]\n",
    "\n",
    "        print(f'executing {N_CHUNKS} commands in chunks of {N_PROCS} parallel commands')\n",
    "\n",
    "        for cmd_grp in grouper(tqdm(commands), N_PROCS, ''):\n",
    "            processes = [subprocess.Popen(cmd, shell=True) for cmd in cmd_grp]\n",
    "            for p in processes:\n",
    "                p.wait()\n",
    "\n",
    "            # we just saved 5 chunks in two places -- the hf cache directory\n",
    "            # and the local directory. to avoid running out of disk space, we\n",
    "            # will wipe the cache directory after every proc group\n",
    "            hf_cache_dir = os.path.join(os.path.expanduser('~'), '.cache', 'huggingface', 'datasets')\n",
    "            if os.path.isdir(hf_cache_dir):\n",
    "                shutil.rmtree(hf_cache_dir)\n",
    "\n",
    "        base_dataset = datasets.DatasetDict({\n",
    "            split: datasets.concatenate_datasets(\n",
    "                [datasets.load_from_disk(f)[split]\n",
    "                 for f in glob.glob('mtg-mlm-nsp-dataset-chunks/*')])\n",
    "            for split in SPLITS})\n",
    "    else:\n",
    "        tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "        tokenizer_map_func = build_tokenizer_map_func(tokenizer, max_length=MAX_SEQ_LENGTH)\n",
    "        \n",
    "        base_dataset = (datasets.load_dataset('edhrec_dataset.py',\n",
    "                                              data_dir=DATA_DIR,\n",
    "                                              num_max_pairs=NUM_MAX_PAIRS)\n",
    "                        .map(tokenizer_map_func, batched=False, num_proc=N_PROCS)\n",
    "                        .map(add_labels, batched=True, num_proc=N_PROCS))\n",
    "\n",
    "    dataset = (base_dataset\n",
    "               .shuffle(seeds={split: SEED for split in SPLITS}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset if MAKE_MLM_NSP_DATASET else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset if MAKE_MLM_NSP_DATASET else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.shape if MAKE_MLM_NSP_DATASET else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_DATASET_TO_FILE:\n",
    "    dataset.save_to_disk('mtg-mlm-nsp-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "aws s3 ls s3://mtg-research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the smaller dataset defined above,\n",
    "\n",
    "+ ~~add the tokenizer~~\n",
    "+ ~~build the trainer and train config~~\n",
    "+ ~~do a train round with the smaller datasets~~\n",
    "+ ~~any difference at all??~~\n",
    "\n",
    "if it looks promising,\n",
    "\n",
    "+ build a *real* dataset\n",
    "    + ~~write chunked dataset processer to leverage multiple cpus~~\n",
    "    + move to a **CPU** box -- not a GPU box. this dataset creation is done on the CPU.\n",
    "    + set up cpu box\n",
    "        + `ssh-keygen -t ed25519 -C \"r.zach.lamberty@gmail.com\"`\n",
    "        + `cat ~/.ssh/id_ed25519.pub`\n",
    "        + add to github [here](https://github.com/settings/keys)\n",
    "        + `git clone git@github.com:RZachLamberty/mtg-research.git`\n",
    "        + `source activate ...`\n",
    "        + `pip install datasets transformers`\n",
    "        + `jupyter notebook --no-browser --ip=0.0.0.0`\n",
    "    + locally\n",
    "        + `ssh -NfL 9999:localhost:8888 rzlcpu`\n",
    "        + https://localhost:9999\n",
    "        + `scp -r mtg-language rzlcpu:~/mtg-research/bert-edh-pair-prediction/mtg-language`\n",
    "        + `scp -r mtg-language-data rzlcpu:~/mtg-research/bert-edh-pair-prediction/mtg-language-data`\n",
    "    + in the jupyter notebook, update the flags at the top\n",
    "        + `True`:\n",
    "            + `MAKE_MLM_NSP_DATASET`\n",
    "            + `DO_CHUNKED_DATASET_BUILDING`\n",
    "            + `SAVE_DATASET_TO_FILE`\n",
    "        + all others `False`\n",
    "    + save this dataset and copy it down\n",
    "+ train\n",
    "    + move to GPU (upload the saved dataset\n",
    "    + **add early stopping**\n",
    "    + run it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-training\n",
    "\n",
    "we actually do care most about the nsp (next sentence prediction) task -- for us, that's the \"is edhrec pair / isn't edhrec pair\" concept. this means we *have* to do a `bert` model, because all of the other models dropped that task in favor of others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_PRE_TRAINING:\n",
    "    config = BertConfig(vocab_size=VOCAB_SIZE)\n",
    "    model = BertForPreTraining(config=config)\n",
    "    print(f'{model.num_parameters():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_PRE_TRAINING:\n",
    "    if DO_SMALL_PRETRAIN_TEST:\n",
    "        TRAIN_BATCH = 8\n",
    "        EVAL_BATCH = 8\n",
    "        LOGGING_STEPS = 250\n",
    "        train_dataset = dataset['train'].select(range(80))\n",
    "        eval_dataset = dataset['validation'].select(range(80))\n",
    "    else:\n",
    "        TRAIN_BATCH = 36\n",
    "        EVAL_BATCH = 36\n",
    "        LOGGING_STEPS = 250\n",
    "        train_dataset = dataset['train']\n",
    "        eval_dataset = dataset['validation']\n",
    "\n",
    "    output_dir = './mtg-language-results-v1'\n",
    "    \n",
    "    # # use this format to pick up from an aborted run\n",
    "    # model = BertForPreTraining.from_pretrained(f'./{output_dir}/checkpoint-5750')\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,                    # output directory\n",
    "        num_train_epochs=1,                       # total # of training epochs\n",
    "        per_device_train_batch_size=TRAIN_BATCH,  # batch size per device during training\n",
    "        per_device_eval_batch_size=EVAL_BATCH,    # batch size for evaluation\n",
    "        warmup_steps=500,                         # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,                        # strength of weight decay\n",
    "        logging_dir='./logs',                     # directory for storing logs\n",
    "        # my custom ones\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        overwrite_output_dir=True,\n",
    "        evaluation_strategy='steps',\n",
    "        logging_first_step=True,\n",
    "        seed=1337,\n",
    "        dataloader_drop_last=True,\n",
    "        dataloader_num_workers=30,\n",
    "        label_names=['labels', 'next_sentence_label'],\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,                  # the instantiated ðŸ¤— Transformers model to be trained\n",
    "        args=training_args,           # training arguments, defined above\n",
    "        train_dataset=train_dataset,  # training dataset\n",
    "        eval_dataset=eval_dataset,    # evaluation dataset\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_PRE_TRAINING:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(train_dataset) if DO_PRE_TRAINING else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(eval_dataset) if DO_PRE_TRAINING else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('mtg-language-test-small') if DO_PRE_TRAINING else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\n",
    "    'fill-mask',\n",
    "    model='./mtg-language-test-small',\n",
    "    tokenizer=tokenizer) if DO_PRE_TRAINING else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['validation'][0]['text_a'] if DO_PRE_TRAINING else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask('{3}{w}{b} legendary creature â€” vampire knight 4/4: vigilance, lifelink {t}, pay 7 life: destroy target nonland [MASK]. activate this ability only during your turn.') if DO_PRE_TRAINING else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
